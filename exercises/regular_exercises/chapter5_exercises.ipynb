{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917c6fea",
   "metadata": {},
   "source": [
    "# Handbook of Solutions: Chapter 5 Exercises (Abraham-Silberschatz OS Concepts 10th Ed.)\n",
    "\n",
    "## **Exercise 5.1: Number of Possible Schedules**\n",
    "\n",
    "### **Question**\n",
    "A CPU-scheduling algorithm determines an order for the execution of its scheduled processes. Given `n` processes to be scheduled on one processor, how many different schedules are possible? Give a formula in terms of `n`.\n",
    "\n",
    "### **Answer**\n",
    "`n!` (n factorial).\n",
    "\n",
    "### **Detailed Explanation**\n",
    "For a **single processor system** with `n` processes, a schedule is a **permutation** (a specific sequence) of all `n` processes. The number of distinct ways to order `n` distinct items is calculated by the factorial function:\n",
    "\\[\n",
    "n! = n \\times (n-1) \\times (n-2) \\times ... \\times 2 \\times 1\n",
    "\\]\n",
    "**Reasoning:** You have `n` choices for which process runs first. Once that process is chosen, you have `(n-1)` remaining choices for the second process, then `(n-2)` for the third, and so on, until only one process remains. This combinatorial principle is fundamental to understanding the vast solution space scheduling algorithms navigate.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.2: Preemptive vs. Nonpreemptive Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Explain the difference between preemptive and nonpreemptive scheduling.\n",
    "\n",
    "### **Answer**\n",
    "*   **Preemptive Scheduling:** Allows the operating system to interrupt a currently running process before it completes its CPU burst, forcibly reallocating the CPU to another process.\n",
    "*   **Nonpreemptive Scheduling (or Cooperative Scheduling):** Once a process is allocated the CPU, it retains control until it terminates or voluntarily yields the CPU (e.g., by waiting for an I/O request).\n",
    "\n",
    "### **Detailed Explanation**\n",
    "This distinction is central to CPU scheduling design and has major implications for system behavior:\n",
    "\n",
    "| Feature | Preemptive Scheduling | Nonpreemptive Scheduling |\n",
    "| :--- | :--- | :--- |\n",
    "| **Interruption** | The OS can force a process to stop. | The OS cannot force a process to stop. |\n",
    "| **Control Transfer** | Can happen at any time (on interrupt, timer expiry, higher-priority arrival). | Happens only when the running process gives up the CPU. |\n",
    "| **Flexibility & Responsiveness** | **High.** Allows the system to be more responsive to high-priority or short tasks. | **Low.** A long CPU-bound process can block all others. |\n",
    "| **Critical Section Problem** | More complex, requires careful synchronization (e.g., mutexes). | Simpler, as process runs uninterrupted. |\n",
    "| **Examples** | RR, SRTF, Preemptive Priority. | FCFS, Nonpreemptive SJF, Nonpreemptive Priority. |\n",
    "\n",
    "**Key Takeaway:** Preemption is essential for **time-sharing systems** where fairness and response time are critical. Nonpreemptive scheduling is simpler but can lead to poor average response times if long processes monopolize the CPU.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.3: Scheduling Analysis (FCFS, SJF, Future-Knowledge)**\n",
    "\n",
    "### **Question**\n",
    "Suppose that the following processes arrive for execution at the times indicated. Each process will run for the amount of time listed. In answering the questions, use **nonpreemptive scheduling**, and base all decisions on the information you have at the time the decision must be made.\n",
    "\n",
    "| Process | Arrival Time | Burst Time |\n",
    "| :--- | :---: | :---: |\n",
    "| P1 | 0.0 | 8 |\n",
    "| P2 | 0.4 | 4 |\n",
    "| P3 | 1.0 | 1 |\n",
    "\n",
    "a. What is the average turnaround time for these processes with the **FCFS** scheduling algorithm?\n",
    "b. What is the average turnaround time for these processes with the **SJF** scheduling algorithm?\n",
    "c. Compute the average turnaround time if the CPU is left idle for the first 1 unit and then **SJF** scheduling is used (\"future-knowledge scheduling\").\n",
    "\n",
    "### **Answer**\n",
    "a. **10.53**\n",
    "b. **9.53**\n",
    "c. **6.86**\n",
    "\n",
    "### **Detailed Calculation and Explanation**\n",
    "**Turnaround Time (TAT)** for a process is defined as: `Completion Time - Arrival Time`. The **Average Turnaround Time** is the mean of the TATs for all processes.\n",
    "\n",
    "#### **a. First-Come, First-Served (FCFS)**\n",
    "Processes are executed in order of arrival. *Nonpreemptive.*\n",
    "\n",
    "**Gantt Chart:**\n",
    "```\n",
    "|     P1     |   P2   | P3 |\n",
    "0          8.0      12.0   13.0\n",
    "```\n",
    "**Calculations:**\n",
    "*   **P1:** Arrives at 0.0, starts at 0.0, finishes at 8.0. **TAT₁ = 8.0 - 0.0 = 8.0**\n",
    "*   **P2:** Arrives at 0.4. Must wait for P1 to finish at 8.0. Starts at 8.0, finishes at 12.0. **TAT₂ = 12.0 - 0.4 = 11.6**\n",
    "*   **P3:** Arrives at 1.0. Waits for P1 & P2. Starts at 12.0, finishes at 13.0. **TAT₃ = 13.0 - 1.0 = 12.0**\n",
    "\n",
    "**Average TAT = (8.0 + 11.6 + 12.0) / 3 = 31.6 / 3 = 10.53**\n",
    "\n",
    "#### **b. Shortest-Job-First (SJF) - Nonpreemptive**\n",
    "At time 0.0, only P1 is present, so it runs. When it finishes at time 8.0, we look at the ready queue: P2 (burst 4) and P3 (burst 1) have arrived. SJF picks the shortest one (P3), then P2.\n",
    "\n",
    "**Gantt Chart:**\n",
    "```\n",
    "|     P1     | P3 |   P2   |\n",
    "0          8.0   9.0      13.0\n",
    "```\n",
    "**Calculations:**\n",
    "*   **P1:** Starts 0.0, finishes 8.0. **TAT₁ = 8.0 - 0.0 = 8.0**\n",
    "*   **P3:** Arrives 1.0. Waits for P1. Starts 8.0, finishes 9.0. **TAT₃ = 9.0 - 1.0 = 8.0**\n",
    "*   **P2:** Arrives 0.4. Waits for P1 and P3. Starts 9.0, finishes 13.0. **TAT₂ = 13.0 - 0.4 = 12.6**\n",
    "\n",
    "**Average TAT = (8.0 + 12.6 + 8.0) / 3 = 28.6 / 3 = 9.53**\n",
    "\n",
    "#### **c. Future-Knowledge SJF (Idle for 1 unit)**\n",
    "The scheduler knows the future: it idles the CPU until time 1.0 when all three processes have arrived. *Then* it applies nonpreemptive SJF on the set {P1(8), P2(4), P3(1)}. The shortest is P3, then P2, then P1.\n",
    "\n",
    "**Gantt Chart:**\n",
    "```\n",
    "| IDLE | P3 |   P2   |     P1     |\n",
    "0    1.0 2.0     6.0          14.0\n",
    "```\n",
    "**Calculations:**\n",
    "*   **P3:** Arrives 1.0, starts 1.0, finishes 2.0. **TAT₃ = 2.0 - 1.0 = 1.0**\n",
    "*   **P2:** Arrives 0.4, starts 2.0, finishes 6.0. **TAT₂ = 6.0 - 0.4 = 5.6**\n",
    "*   **P1:** Arrives 0.0, starts 6.0, finishes 14.0. **TAT₁ = 14.0 - 0.0 = 14.0**\n",
    "\n",
    "**Average TAT = (1.0 + 5.6 + 14.0) / 3 = 20.6 / 3 = 6.86**\n",
    "\n",
    "**Insight:** This demonstrates the **ideal** scenario for SJF and why it's optimal for minimizing average waiting/turnaround time, but it's impossible to implement without perfect future knowledge. Real algorithms (like SRTF) try to approximate this using preemption based on estimated remaining time.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.4: Comprehensive Scheduling Algorithm Comparison**\n",
    "\n",
    "### **Question**\n",
    "Consider the following set of processes, with the length of the CPU burst time given in milliseconds:\n",
    "\n",
    "| Process | Burst Time | Priority |\n",
    "| :--- | :---: | :---: |\n",
    "| P1 | 2 | 2 |\n",
    "| P2 | 1 | 1 |\n",
    "| P3 | 8 | 4 |\n",
    "| P4 | 4 | 2 |\n",
    "| P5 | 5 | 3 |\n",
    "\n",
    "The processes are assumed to have arrived in the order P1, P2, P3, P4, P5, **all at time 0**.\n",
    "\n",
    "a. Draw four Gantt charts for: FCFS, SJF, **nonpreemptive priority** (larger number = higher priority), and RR (quantum = 2).\n",
    "b. What is the turnaround time of each process for each algorithm?\n",
    "c. What is the waiting time of each process for each algorithm?\n",
    "d. Which algorithm results in the minimum average waiting time?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Gantt Charts**\n",
    "\n",
    "1.  **FCFS (First-Come, First-Served):** Execute in arrival order (P1, P2, P3, P4, P5).\n",
    "    ```\n",
    "    | P1 | P2 |   P3   |  P4  |   P5   |\n",
    "    0   2   3    11      15     20\n",
    "    ```\n",
    "\n",
    "2.  **SJF (Shortest-Job-First) - Nonpreemptive:** At time 0, choose the process with the shortest burst from all available. Ties broken by arrival order (FCFS).\n",
    "    *   Time 0: Shortest is **P2 (1)**.\n",
    "    *   After P2 (time 1): Shortest is **P1 (2)**.\n",
    "    *   After P1 (time 3): Shortest is **P4 (4)**.\n",
    "    *   After P4 (time 7): Shortest is **P5 (5)**.\n",
    "    *   After P5 (time 12): Run **P3 (8)**.\n",
    "    ```\n",
    "    | P2 | P1 |  P4  |   P5   |   P3   |\n",
    "    0   1   3      7       12       20\n",
    "    ```\n",
    "\n",
    "3.  **Nonpreemptive Priority (Higher number = Higher priority):** At time 0, choose the highest priority process. Ties broken by arrival order.\n",
    "    *   Time 0: Priorities: P1(2), P2(1), P3(4), P4(2), P5(3). Highest is **P3 (4)**.\n",
    "    *   After P3 (time 8): Highest priority among remaining is **P5 (3)**.\n",
    "    *   After P5 (time 13): Tied priorities P1(2) and P4(2). Choose **P1** (arrived first).\n",
    "    *   After P1 (time 15): Run **P4 (2)**.\n",
    "    *   After P4 (time 19): Run **P2 (1)**.\n",
    "    ```\n",
    "    |     P3     |   P5   | P1 | P4 | P2 |\n",
    "    0          8       13     15   19   20\n",
    "    ```\n",
    "\n",
    "4.  **RR (Round Robin, Quantum = 2):** Processes are cycled through in a FIFO queue. A process is preempted after 2 ms if it hasn't finished.\n",
    "    **Queue Evolution and Gantt Chart:**\n",
    "    *   Time 0: Queue: [P1, P2, P3, P4, P5]. Run **P1** for 2 ms (finishes).\n",
    "    *   Time 2: Queue: [P2, P3, P4, P5]. Run **P2** for 1 ms (finishes, used less than quantum).\n",
    "    *   Time 3: Queue: [P3, P4, P5]. Run **P3** for 2 ms (preempted, 6 ms left).\n",
    "    *   Time 5: Queue: [P4, P5, P3]. Run **P4** for 2 ms (preempted, 2 ms left).\n",
    "    *   Time 7: Queue: [P5, P3, P4]. Run **P5** for 2 ms (preempted, 3 ms left).\n",
    "    *   Time 9: Queue: [P3, P4, P5]. Run **P3** for 2 ms (preempted, 4 ms left).\n",
    "    *   Time 11: Queue: [P4, P5, P3]. Run **P4** for 2 ms (finishes).\n",
    "    *   Time 13: Queue: [P5, P3]. Run **P5** for 2 ms (preempted, 1 ms left).\n",
    "    *   Time 15: Queue: [P3, P5]. Run **P3** for 2 ms (preempted, 2 ms left).\n",
    "    *   Time 17: Queue: [P5, P3]. Run **P5** for 1 ms (finishes).\n",
    "    *   Time 18: Queue: [P3]. Run **P3** for 2 ms (preempted/continues, 0 ms left? Let's track).\n",
    "        Let's track burst times remaining:\n",
    "        P3: Start 8, after (3-5): 6 left, after (9-11): 4 left, after (15-17): 2 left. At time 18, it runs its last 2 ms.\n",
    "    *   Time 20: **P3** finishes.\n",
    "    ```\n",
    "    |P1|P2| P3 | P4 | P5 | P3 | P4 | P5 | P3 | P5 | P3 |\n",
    "    0 2 3  5   7   9   11  13  15  17  18  20\n",
    "    ```\n",
    "    *Simplified visual Gantt:*\n",
    "    ```\n",
    "    P1 P2 P3 P4 P5 P3 P4 P5 P3 P5 P3\n",
    "    0  2  3  5  7  9 11 13 15 17 18 20\n",
    "    ```\n",
    "\n",
    "#### **b. Turnaround Time Table (Completion Time - Arrival Time)**\n",
    "Since all processes arrive at time 0, TAT = Completion Time.\n",
    "\n",
    "| Process | FCFS | SJF | Priority | RR |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| P1 | 2 | 3 | 15 | 2 |\n",
    "| P2 | 3 | 1 | 20 | 3 |\n",
    "| P3 | 11 | 20 | 8 | 20 |\n",
    "| P4 | 15 | 7 | 19 | 13 |\n",
    "| P5 | 20 | 12 | 13 | 18 |\n",
    "\n",
    "#### **c. Waiting Time Table (Turnaround Time - Burst Time)**\n",
    "*Waiting Time = TAT - Burst Time*\n",
    "\n",
    "| Process | FCFS | SJF | Priority | RR |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| P1 | 0 | 1 | 13 | 0 |\n",
    "| P2 | 2 | 0 | 19 | 2 |\n",
    "| P3 | 3 | 12 | 0 | 12 |\n",
    "| P4 | 11 | 3 | 15 | 9 |\n",
    "| P5 | 15 | 7 | 8 | 13 |\n",
    "\n",
    "**Calculations Example (FCFS P4):** Arrives 0, starts at time 3 (after P1+P2+P3), finishes at 15. **TAT=15**, **Burst=4**, so **Waiting = 15 - 4 = 11**.\n",
    "\n",
    "#### **d. Algorithm with Minimum Average Waiting Time**\n",
    "Calculate the average for each algorithm from the table in part (c):\n",
    "*   **FCFS:** (0+2+3+11+15)/5 = 31/5 = **6.2**\n",
    "*   **SJF:** (1+0+12+3+7)/5 = 23/5 = **4.6**\n",
    "*   **Priority:** (13+19+0+15+8)/5 = 55/5 = **11.0**\n",
    "*   **RR:** (0+2+12+9+13)/5 = 36/5 = **7.2**\n",
    "\n",
    "**SJF** results in the **minimum average waiting time (4.6 ms)**, which aligns with the theoretical proof that **nonpreemptive SJF is optimal for minimizing average waiting time** given a set of processes at time 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54939a95",
   "metadata": {},
   "source": [
    "\n",
    "## **Exercise 5.5: Complex Preemptive Round Robin with Priority**\n",
    "\n",
    "### **Question**\n",
    "The following processes are being scheduled using a **preemptive, round-robin** scheduling algorithm.\n",
    "\n",
    "| Process | Priority | Burst | Arrival |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| P1 | 40 | 20 | 0 |\n",
    "| P2 | 30 | 25 | 25 |\n",
    "| P3 | 30 | 25 | 30 |\n",
    "| P4 | 35 | 15 | 60 |\n",
    "| P5 | 5 | 10 | 100 |\n",
    "| P6 | 10 | 10 | 105 |\n",
    "\n",
    "* Higher priority number = higher relative priority.\n",
    "* **Idle task (P_idle):** Priority 0, runs when no other processes are available.\n",
    "* Time quantum = 10 units.\n",
    "* Rule: If preempted by a higher-priority process, the preempted process goes to the **end** of the queue.\n",
    "\n",
    "**Tasks:**\n",
    "a. Show the scheduling order using a Gantt chart.\n",
    "b. Find the turnaround time for each process.\n",
    "c. Find the waiting time for each process.\n",
    "d. Calculate the CPU utilization rate.\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Gantt Chart**\n",
    "The scheduling order is as follows. This accounts for arrivals, preemptions by higher priority, and the round-robin quantum among equal-priority processes.\n",
    "\n",
    "```\n",
    "Process: P1     | IDLE | P2    | P3    | P2     | P3     | P4           | P2    | P3     | IDLE | P5     | P6      | P5    |\n",
    "Time:    0───20  20──25 25──35  35──45  45──55   55──60   60────────75   75──80  80──90   90──100 100──105 105──115  115──120\n",
    "```\n",
    "\n",
    "**Key Scheduling Decisions Explained:**\n",
    "\n",
    "1.  **0-20:** P1 (priority 40) runs. It's the only process and has highest priority. It completes its full burst of 20.\n",
    "2.  **20-25:** IDLE. No processes in ready queue until P2 arrives at 25.\n",
    "3.  **25-35:** P2 arrives (priority 30) and runs for a full quantum (10). At time 30, P3 arrives (also priority 30) but doesn't preempt as priorities are equal.\n",
    "4.  **35-45:** After P2's quantum expires, we apply RR among equal-priority ready processes (P2 with 15 left, P3 with 25). P3 runs for a quantum (10).\n",
    "5.  **45-55:** P2 runs for another quantum (10). It now has 5 remaining.\n",
    "6.  **55-60:** P3 runs. At time 60, P4 arrives with priority 35 (higher than P3's 30), so P3 is **preempted** after only 5 ms of its quantum and placed at the end of the queue. P3 now has 10 remaining.\n",
    "7.  **60-75:** P4 (highest priority) runs. It could run for a quantum of 10 (60-70), but at 70, no higher-priority process exists, and it continues with its remaining 5 ms, finishing at 75.\n",
    "8.  **75-80:** The scheduler checks the ready queue: P2 (5 left, priority 30), P3 (10 left, priority 30). P2 runs to completion.\n",
    "9.  **80-90:** P3 runs to completion.\n",
    "10. **90-100:** IDLE. No processes until P5 at 100.\n",
    "11. **100-105:** P5 (priority 5) runs. At 105, P6 arrives with priority 10 (higher than 5). P5 is **preempted** and placed at queue end. P5 has 5 remaining.\n",
    "12. **105-115:** P6 (highest priority) runs to completion (burst 10).\n",
    "13. **115-120:** P5 (only remaining) runs its final 5 ms.\n",
    "\n",
    "#### **b. Turnaround Time (TAT)**\n",
    "*Turnaround Time = Completion Time - Arrival Time*\n",
    "\n",
    "| Process | Arrival | Completion | TAT Calculation | TAT |\n",
    "| :--- | :---: | :---: | :--- | :---: |\n",
    "| P1 | 0 | 20 | 20 - 0 | **20** |\n",
    "| P2 | 25 | 80 | 80 - 25 | **55** |\n",
    "| P3 | 30 | 90 | 90 - 30 | **60** |\n",
    "| P4 | 60 | 75 | 75 - 60 | **15** |\n",
    "| P5 | 100 | 120 | 120 - 100 | **20** |\n",
    "| P6 | 105 | 115 | 115 - 105 | **10** |\n",
    "\n",
    "#### **c. Waiting Time (WT)**\n",
    "*Waiting Time = Turnaround Time - Burst Time* (Time spent ready but not executing)\n",
    "\n",
    "| Process | TAT | Burst | WT Calculation | WT |\n",
    "| :--- | :---: | :---: | :--- | :---: |\n",
    "| P1 | 20 | 20 | 20 - 20 | **0** |\n",
    "| P2 | 55 | 25 | 55 - 25 | **30** |\n",
    "| P3 | 60 | 25 | 60 - 25 | **35** |\n",
    "| P4 | 15 | 15 | 15 - 15 | **0** |\n",
    "| P5 | 20 | 10 | 20 - 10 | **10** |\n",
    "| P6 | 10 | 10 | 10 - 10 | **0** |\n",
    "\n",
    "*Note: The textbook answer lists P2's waiting time as 40, which appears to be an error based on the standard definition (TAT - Burst). The calculation above (30) is consistent with the simulated Gantt chart.*\n",
    "\n",
    "#### **d. CPU Utilization Rate**\n",
    "*CPU Utilization = (Total time CPU is busy) / (Total observation time)*\n",
    "\n",
    "*   **Total Observation Time:** 0 to 120 = **120 units**\n",
    "*   **Total Busy Time:** Sum of all process bursts = 20+25+25+15+10+10 = **105 units**\n",
    "*   **Idle Time:** 120 - 105 = **15 units** (from 20-25 and 90-100)\n",
    "\n",
    "\\[\n",
    "\\text{CPU Utilization} = \\frac{105}{120} = 0.875 = \\mathbf{87.5\\%}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.6: Multilevel Queueing with Different Time Quanta**\n",
    "\n",
    "### **Question**\n",
    "What advantage is there in having different time-quantum sizes at different levels of a multilevel queueing system?\n",
    "\n",
    "### **Answer**\n",
    "The primary advantage is **efficiency and performance optimization tailored to different process types**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "A **Multilevel Queue (MLQ)** algorithm partitions the ready queue into several separate queues, each with its own scheduling algorithm and priority. Assigning different time quanta to these queues allows the OS to match scheduling behavior to process characteristics:\n",
    "\n",
    "1.  **High-Priority, Interactive Queues (e.g., foreground processes):** Given a **small time quantum** (e.g., 10-100 ms).\n",
    "    *   **Why:** Interactive processes (editors, shells) require frequent, short CPU bursts to remain responsive to user input.\n",
    "    *   **Benefit:** Small quanta ensure quick cycling among these processes, yielding good response times and a smooth user experience.\n",
    "\n",
    "2.  **Low-Priority, Batch Queues (e.g., background processes):** Given a **large time quantum** or even FCFS.\n",
    "    *   **Why:** CPU-bound batch jobs (scientific computations, compilations) benefit from long, uninterrupted runs to minimize overhead.\n",
    "    *   **Benefit:** Large quanta drastically reduce the frequency of **context switches**, which are costly operations. This allows these processes to make more efficient use of the CPU and complete faster.\n",
    "\n",
    "**Overall System Benefit:** This design provides a balanced system that delivers excellent interactive performance while also maximizing throughput for batch jobs, optimizing overall system efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.7: Relationships Between Algorithm Sets**\n",
    "\n",
    "### **Question**\n",
    "CPU-scheduling algorithms are often parameterized, forming sets of algorithms (e.g., all possible RR time quanta). One set may include another. What relation holds between the following pairs?\n",
    "\n",
    "### **Answer & Explanation**\n",
    "\n",
    "#### **a. Priority and SJF**\n",
    "*   **Relation:** **SJF is a special case of Priority scheduling.**\n",
    "*   **Explanation:** In Priority scheduling, the process with the highest priority number is selected. If we define a process's priority as the *inverse* of its predicted next CPU burst length (shorter burst = higher priority), then Priority scheduling becomes SJF.\n",
    "\n",
    "#### **b. Multilevel Feedback Queues (MLFQ) and FCFS**\n",
    "*   **Relation:** **FCFS is often used in at least one queue of an MLFQ system.**\n",
    "*   **Explanation:** MLFQ typically uses RR with increasing time quanta in higher-level queues for interactive processes, and often employs **FCFS in the lowest-priority queue** for long-running CPU-bound background jobs. Thus, FCFS is contained within the set of possible MLFQ configurations.\n",
    "\n",
    "#### **c. Priority and FCFS**\n",
    "*   **Relation:** **FCFS can be seen as a special case of Priority scheduling.**\n",
    "*   **Explanation:** If we define a process's priority as its *arrival time* (earlier arrival = higher priority), then the Priority scheduling algorithm will always select the process that arrived first, which is exactly FCFS.\n",
    "\n",
    "#### **d. RR and SJF**\n",
    "*   **Relation:** **No direct subset relationship.**\n",
    "*   **Explanation:** RR is designed for fairness and preemption based on time, while SJF is designed for optimal average waiting time and can be non-preemptive or preemptive (SRTF). They are based on fundamentally different criteria (time slices vs. burst lengths). One is not a parameterized version of the other.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.8: Favoring I/O-Bound Processes**\n",
    "\n",
    "### **Question**\n",
    "Suppose that a CPU scheduling algorithm favors those processes that have used the least processor time in the recent past. Why will this algorithm favor I/O-bound programs and yet not permanently starve CPU-bound programs?\n",
    "\n",
    "### **Answer**\n",
    "This algorithm approximates **Shortest-Remaining-Time-First (SRTF)** or is similar to the logic in a **Multilevel Feedback Queue (MLFQ)**, where processes that relinquish the CPU quickly (for I/O) get priority boosts.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "| Process Type | Characteristic | Under this Algorithm |\n",
    "| :--- | :--- | :--- |\n",
    "| **I/O-Bound** | Many short CPU bursts (quickly issues I/O op and blocks). | After completing a short burst and performing I/O, it returns to ready queue with **very little recent CPU time used**, making it high priority for the next scheduling decision. |\n",
    "| **CPU-Bound** | Long CPU bursts (computes for extended periods). | Will accumulate \"recent processor time,\" causing its priority to gradually lower. |\n",
    "\n",
    "**Why Starvation Doesn't Occur (Aging):**\n",
    "1.  While the CPU-bound process's priority decreases, the I/O-bound processes will **frequently block** for I/O.\n",
    "2.  During these I/O waits, the CPU-bound process, even with low priority, will eventually become the **only ready process** and will be scheduled.\n",
    "3.  Furthermore, practical implementations often incorporate **aging** – gradually increasing the priority of processes that have waited a long time – which guarantees that CPU-bound jobs will eventually receive CPU time.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.9: PCS vs. SCS Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Distinguish between PCS and SCS scheduling.\n",
    "\n",
    "### **Answer**\n",
    "*   **PCS (Process-Contention Scope):** Scheduling competition occurs **among threads belonging to the same process**. It is performed by the **thread library** at user-level.\n",
    "*   **SCS (System-Contention Scope):** Scheduling competition occurs **among all threads in the system**. It is performed by the **operating system kernel**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "This distinction is crucial in multithreading models:\n",
    "\n",
    "| Feature | PCS (User-Level) | SCS (Kernel-Level) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Scheduler** | Thread library (e.g., pthreads). | OS kernel. |\n",
    "| **Scope** | Local to the process. Threads compete for LWPs. | System-wide. Kernel threads compete for CPUs. |\n",
    "| **Preemption** | Typically cooperative (non-preemptive) unless the library implements preemptive signals. | Preemptive, based on OS interrupts and time slices. |\n",
    "| **Models** | Used in **Many-to-One** and **Many-to-Many** threading models. | Used in **One-to-One** model. In **Many-to-Many**, the kernel schedules LWPs (kernel threads) via SCS. |\n",
    "| **Relation** | In Many-to-Many, PCS determines which user thread runs on an available LWP; SCS determines which LWP gets a CPU. | In the **One-to-One** model, PCS and SCS are effectively the same because each user thread maps to a kernel thread. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.10: Traditional UNIX Scheduler Priority Calculation**\n",
    "\n",
    "### **Question**\n",
    "The traditional UNIX scheduler uses: `Priority = (recent CPU usage / 2) + base` where `base = 60`. Higher numbers mean **lower** priority.\n",
    "Given: recent CPU usage for P1=40, P2=18, P3=10. What are the new priorities? Based on this, does the scheduler raise or lower the relative priority of a CPU-bound process?\n",
    "\n",
    "### **Answer**\n",
    "*   **P1:** `(40/2) + 60 = 20 + 60 = 80`\n",
    "*   **P2:** `(18/2) + 60 = 9 + 60 = 69`\n",
    "*   **P3:** `(10/2) + 60 = 5 + 60 = 65`\n",
    "\n",
    "The scheduler **lowers** the relative priority (i.e., gives a higher priority number) of a CPU-bound process.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "*   **Inverse Priority Scale:** Remember, in this system, **a lower priority number means higher scheduling priority**.\n",
    "*   **CPU-Bound Process (e.g., P1):** Uses the CPU heavily → high \"recent CPU usage\" value → leads to a higher calculated priority number → **lower scheduling priority**. This **penalizes** CPU-bound processes.\n",
    "*   **I/O-Bound Process (e.g., P3):** Uses the CPU sparingly → low \"recent CPU usage\" → lower calculated priority number → **higher scheduling priority**. This **rewards** interactive/I/O-bound processes.\n",
    "*   **Dynamic Feedback:** This formula is recalculated periodically (once per second), creating a **feedback loop**. A process that has recently used a lot of CPU will be penalized in the next period, encouraging CPU time sharing and improving response time for interactive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997accf8",
   "metadata": {},
   "source": [
    "# Handbook of Solutions: Chapter 5 Exercises (Abraham-Silberschatz OS Concepts 10th Ed.)\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.11: Context Switches for I/O-Bound vs. CPU-Bound Programs**\n",
    "\n",
    "### **Question**\n",
    "Of these two types of programs:\n",
    "a. I/O-bound\n",
    "b. CPU-bound\n",
    "\n",
    "which is more likely to have voluntary context switches, and which is more likely to have nonvoluntary context switches? Explain your answer.\n",
    "\n",
    "### **Answer**\n",
    "*   **I/O-bound programs** are more likely to have **voluntary context switches**.\n",
    "*   **CPU-bound programs** are more likely to have **nonvoluntary context switches**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "A **context switch** is the process of saving the state of one process and loading the saved state of another. They are categorized by their cause:\n",
    "\n",
    "| Type | Cause | How it Happens |\n",
    "| :--- | :--- | :--- |\n",
    "| **Voluntary** | The running process **gives up the CPU willingly**. | Process issues a system call (e.g., for I/O, `wait()`, `yield()`). |\n",
    "| **Nonvoluntary (Preemptive)** | The OS **forcibly removes the process** from the CPU. | Interrupt occurs (timer expiration, higher-priority process arrival). |\n",
    "\n",
    "**I/O-Bound Programs:**\n",
    "*   **Characteristic:** Frequent, short CPU bursts followed by I/O operations (e.g., reading from disk, waiting for user input).\n",
    "*   **Behavior:** After a short CPU burst, the program will issue an I/O request (like `read()`). This system call triggers a transition to the **waiting** state, causing a **voluntary context switch**.\n",
    "*   **Conclusion:** Their execution pattern naturally leads to many voluntary context switches.\n",
    "\n",
    "**CPU-Bound Programs:**\n",
    "*   **Characteristic:** Long CPU bursts with infrequent I/O operations (e.g., scientific calculations, matrix multiplication).\n",
    "*   **Behavior:** They rarely issue I/O requests and thus rarely give up the CPU voluntarily. To prevent them from monopolizing the CPU and ensure fairness, the OS relies on **timer interrupts**. When the time quantum (in RR) expires or a higher-priority process arrives, the OS performs a **nonvoluntary context switch**.\n",
    "*   **Conclusion:** They are primarily preempted by the OS scheduler, leading to nonvoluntary context switches.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.12: Conflicting Scheduling Criteria**\n",
    "\n",
    "### **Question**\n",
    "Discuss how the following pairs of scheduling criteria conflict in certain settings.\n",
    "\n",
    "### **Answer & Explanation**\n",
    "\n",
    "Scheduling algorithm design involves trade-offs. Optimizing for one metric often degrades another.\n",
    "\n",
    "#### **a. CPU Utilization and Response Time**\n",
    "*   **Conflict:** Maximizing CPU utilization can degrade response time, and vice-versa.\n",
    "*   **Explanation:**\n",
    "    *   **High CPU Utilization** is achieved by keeping the CPU busy at all times, often by favoring **CPU-bound processes** with long bursts or by minimizing context switches.\n",
    "    *   **Good Response Time** (important for interactive systems) requires the CPU to be frequently available to handle short, incoming requests. This often means **preempting** long-running CPU-bound tasks to service interactive tasks, which introduces context switches and can leave the CPU idle briefly while switching.\n",
    "    *   **Example:** In a **batch processing system** (goal: high utilization), using FCFS might keep the CPU 100% busy but lead to terrible response times for short interactive tasks. In a **time-sharing system** (goal: good response time), using RR with a small time quantum improves responsiveness but increases context switch overhead, potentially lowering overall CPU utilization.\n",
    "\n",
    "#### **b. Average Turnaround Time and Maximum Waiting Time**\n",
    "*   **Conflict:** Minimizing the average turnaround time can lead to an unacceptably high maximum waiting time for some processes (starvation).\n",
    "*   **Explanation:**\n",
    "    *   **Minimizing Average Turnaround Time** is the goal of algorithms like **SJF/SRTF**. They prioritize short jobs, which reduces the average.\n",
    "    *   However, this can cause **indefinite postponement (starvation)** of long jobs. If short jobs keep arriving, a long job may wait indefinitely, causing its waiting time—and the system's **maximum waiting time**—to grow unbounded.\n",
    "    *   **Example:** In a pure nonpreemptive SJF schedule, a single very long job will always be passed over if shorter jobs continue to arrive, maximizing its individual waiting time.\n",
    "\n",
    "#### **c. I/O Device Utilization and CPU Utilization**\n",
    "*   **Conflict:** Over-optimizing for one can lead to under-utilization of the other.\n",
    "*   **Explanation:**\n",
    "    *   **High CPU Utilization** is achieved by running CPU-bound tasks that rarely perform I/O.\n",
    "    *   **High I/O Device Utilization** is achieved by keeping I/O devices busy, which requires having multiple I/O-bound processes that frequently issue I/O requests.\n",
    "    *   The conflict arises in finding a balance. A system full of CPU-bound processes will have high CPU utilization but low I/O utilization (devices sit idle). A system full of I/O-bound processes will keep devices busy but may leave the CPU underutilized because processes are often blocked waiting for I/O.\n",
    "    *   The ideal is a **mixed workload** where when one process is using the CPU, another is using an I/O device, allowing both to be utilized concurrently. However, scheduling to achieve perfect overlap is complex and these metrics can directly conflict if the workload is unbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.13: Implementing Priority in Lottery Scheduling**\n",
    "\n",
    "### **Question**\n",
    "In lottery scheduling, processes hold lottery tickets for CPU allocation. The BTV OS holds 50 lotteries/second, with each winner getting 20 ms. Describe how the BTV scheduler can ensure that higher-priority threads receive more attention from the CPU than lower-priority threads.\n",
    "\n",
    "### **Answer**\n",
    "The scheduler can assign tickets to threads **in proportion to their priority**. Higher-priority threads receive more tickets, increasing their probability of winning each lottery.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "Lottery scheduling is a **probabilistic** algorithm that provides proportional-share resource allocation. The key mechanism is the distribution of tickets.\n",
    "\n",
    "1.  **Assigning Tickets:** Each thread is assigned a number of lottery tickets. This number is not fixed; it is determined by the thread's **priority**. For example:\n",
    "    *   Priority 10 (High): 100 tickets\n",
    "    *   Priority 5 (Medium): 50 tickets\n",
    "    *   Priority 1 (Low): 10 tickets\n",
    "    The total tickets in the system = sum of all threads' tickets.\n",
    "\n",
    "2.  **Holding the Lottery:** When a scheduling decision is needed (50 times per second in BTV), the scheduler picks a random number corresponding to a ticket. The thread holding that winning ticket runs for the next 20 ms time slice.\n",
    "\n",
    "3.  **Mathematical Guarantee:** A thread's probability of winning the lottery (and thus receiving CPU time) is:\n",
    "    $$\n",
    "    P(\\text{thread}) = \\frac{\\text{Tickets held by thread}}{\\text{Total tickets in system}}\n",
    "    $$\n",
    "    Therefore, a thread with twice as many tickets as another has **twice the probability** of being scheduled, ensuring it receives \"more attention\" over time.\n",
    "\n",
    "4.  **Dynamic Adjustment:** The system can dynamically adjust ticket allocations based on changing priorities or to enforce fairness, providing a flexible mechanism for priority-based scheduling without the starvation risks of strict priority scheduling.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.14: Run Queue Design for Multicore Systems**\n",
    "\n",
    "### **Question**\n",
    "On multicore systems, there are two general options for the run queue: (1) per-core run queues, or (2) a single shared run queue. What are the advantages and disadvantages of each?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "| Approach | Advantages | Disadvantages |\n",
    "| :--- | :--- | :--- |\n",
    "| **Per-Core Run Queues** (Each core has its own private queue) | **1. Scalability:** No locking contention for the queue, allowing cores to schedule independently. <br> **2. Cache Locality:** A process tends to run on the same core, making better use of cached data. <br> **3. Reduced Overhead:** No need for complex locking mechanisms for queue access. | **1. Load Imbalance:** One core may be idle while another has a long queue. Requires a **load-balancing** mechanism. <br> **2. Complexity of Migration:** Moving a process from one core's queue to another's (for load balancing) is costly and can break cache locality. |\n",
    "| **Single Shared Run Queue** (One global queue for all cores) | **1. Automatic Load Balancing:** Cores automatically take the next available process, naturally balancing load. <br> **2. Simplicity:** Conceptually simple, easy to implement fairness policies (e.g., global RR). | **1. Scalability Bottleneck:** The single queue becomes a **contended shared resource**. Cores must lock the queue to access it, which can serialize scheduling decisions and hurt performance as core count increases. <br> **2. Poor Cache Locality:** A process may execute on a different core each time it is scheduled, causing **cache misses**. |\n",
    "\n",
    "**Conclusion:** Modern OSes typically use a **hybrid approach** (e.g., per-core queues with periodic load balancing) to mitigate the disadvantages of each pure design.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.15: Implications of Parameters in Exponential Averaging**\n",
    "\n",
    "### **Question**\n",
    "Consider the exponential average formula used to predict the length of the next CPU burst:\n",
    "$$\n",
    "\\tau_{n+1} = \\alpha t_n + (1 - \\alpha) \\tau_n\n",
    "$$\n",
    "Where:\n",
    "*   \\( \\tau_{n+1} \\) = predicted length of next CPU burst\n",
    "*   \\( t_n \\) = actual length of the \\(n\\)th CPU burst\n",
    "*   \\( \\tau_n \\) = previous prediction\n",
    "*   \\( \\alpha \\) = weighting parameter (\\(0 \\leq \\alpha \\leq 1\\))\n",
    "\n",
    "What are the implications of assigning the following values?\n",
    "\n",
    "### **Answer & Explanation**\n",
    "\n",
    "The parameter \\( \\alpha \\) controls the weight given to the **most recent observation** versus the **entire past history**.\n",
    "\n",
    "#### **a. α = 0 and τ₀ = 100 milliseconds**\n",
    "*   **Implication:** **The prediction never changes. History is everything; recent behavior is ignored.**\n",
    "*   **Explanation:**\n",
    "    *   Substituting \\( \\alpha = 0 \\) into the formula: \\( \\tau_{n+1} = 0 \\cdot t_n + (1-0) \\cdot \\tau_n = \\tau_n \\).\n",
    "    *   The new prediction is always equal to the old prediction, regardless of the actual burst length \\( t_n \\).\n",
    "    *   The system will **always use the initial guess \\( \\tau_0 = 100 \\)** for all future predictions. This eliminates the adaptive nature of the algorithm, making it useless for tracking changes in process behavior.\n",
    "\n",
    "#### **b. α = 0.99 and τ₀ = 10 milliseconds**\n",
    "*   **Implication:** **The prediction is overwhelmingly based on the most recent burst. Past history is nearly irrelevant.**\n",
    "*   **Explanation:**\n",
    "    *   Substituting \\( \\alpha = 0.99 \\): \\( \\tau_{n+1} = 0.99 \\cdot t_n + 0.01 \\cdot \\tau_n \\).\n",
    "    *   The new prediction is 99% determined by the actual length of the last CPU burst (\\( t_n \\)) and only 1% by the entire previous history (\\( \\tau_n \\)).\n",
    "    *   The algorithm becomes **extremely responsive to recent changes** but also highly volatile. A single anomalous long or short burst will drastically alter the prediction. The initial guess \\( \\tau_0 = 10 \\) becomes irrelevant after just one or two bursts.\n",
    "    *   This setting is useful if a process's burst lengths are **highly variable** and you want the predictor to quickly adapt to a new pattern. However, it may overreact to noise.\n",
    "\n",
    "**General Insight:** Choosing \\( \\alpha \\) is a trade-off between **stability** (low \\( \\alpha \\), smooth predictions) and **responsiveness** (high \\( \\alpha \\), quick adaptation). A typical value like \\( \\alpha = 0.5 \\) provides a balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f25dfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Exercise 5.16: Regressive Round-Robin Scheduler**\n",
    "\n",
    "### **Question**\n",
    "A variation of round-robin called **regressive round-robin** assigns each process a time quantum and priority. Initial time quantum = 50 ms. Rules:\n",
    "1. If a process uses its **entire quantum** (doesn't block for I/O): \n",
    "   - Add 10 ms to its quantum (max 100 ms)\n",
    "   - Boost its priority\n",
    "2. If a process **blocks before quantum ends**:\n",
    "   - Reduce quantum by 5 ms\n",
    "   - Priority unchanged\n",
    "\n",
    "Which process type (CPU-bound or I/O-bound) does this scheduler favor? Explain.\n",
    "\n",
    "### **Answer**\n",
    "This scheduler **favors I/O-bound processes**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "The scheduler's rules create feedback that rewards I/O-bound behavior and penalizes CPU-bound behavior:\n",
    "\n",
    "| Process Type | Typical Behavior | Effect under Regressive RR | Consequence |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **I/O-Bound** | Frequently blocks for I/O before quantum expires. | Quantum decreases by 5 ms each schedule, but **priority remains same**. | While quantum shrinks, the process maintains its priority level and actually gets scheduled more **frequently** (due to shorter bursts), leading to better responsiveness. |\n",
    "| **CPU-Bound** | Rarely blocks; uses full quantum. | Quantum increases (up to 100 ms), but **priority is boosted** (likely meaning lowered in Linux-style where higher number = lower priority). | Gets longer quanta but at **lower priority**. This means it will be scheduled less often than higher-priority I/O-bound processes. |\n",
    "\n",
    "**Key Insight:** The scheduler achieves **priority inversion** relative to standard RR:\n",
    "- **I/O-bound processes** keep their original (higher) priority and get shorter quanta → they finish their CPU work quickly and return to I/O, getting frequent, responsive service.\n",
    "- **CPU-bound processes** get lower priority and longer quanta → they run for longer periods but less frequently, preventing them from monopolizing the CPU.\n",
    "\n",
    "This design explicitly favors **interactive/I/O-bound processes** by ensuring they don't get stuck behind long CPU bursts, improving overall system responsiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.17: Comprehensive Scheduling Comparison (Second Set)**\n",
    "\n",
    "### **Question**\n",
    "Consider the following processes (all arriving at time 0):\n",
    "\n",
    "| Process | Burst Time | Priority |\n",
    "| :--- | :---: | :---: |\n",
    "| P1 | 5 | 4 |\n",
    "| P2 | 3 | 1 |\n",
    "| P3 | 1 | 2 |\n",
    "| P4 | 7 | 2 |\n",
    "| P5 | 4 | 3 |\n",
    "\n",
    "*(Higher priority number = higher priority)*\n",
    "\n",
    "**Tasks:**\n",
    "a. Draw Gantt charts for: FCFS, SJF, Nonpreemptive Priority, RR (quantum=2).\n",
    "b. Calculate turnaround time for each process under each algorithm.\n",
    "c. Calculate waiting time for each process.\n",
    "d. Identify which algorithm gives minimum average waiting time.\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Gantt Charts**\n",
    "\n",
    "1. **FCFS (First-Come, First-Served)**\n",
    "   ```\n",
    "   | P1 | P2 | P3 | P4  | P5   |\n",
    "   0    5    8    9     16    20\n",
    "   ```\n",
    "\n",
    "2. **SJF (Shortest-Job-First) - Nonpreemptive**\n",
    "   - Order by burst time: P3(1), P2(3), P5(4), P1(5), P4(7)\n",
    "   ```\n",
    "   | P3 | P2 | P5 | P1 | P4  |\n",
    "   0    1    4    8   13    20\n",
    "   ```\n",
    "\n",
    "3. **Nonpreemptive Priority** (Higher number = higher priority)\n",
    "   - Priorities: P1(4), P2(1), P3(2), P4(2), P5(3)\n",
    "   - Order by priority: P1(4), P5(3), then P3 & P4 (both 2, tie-break by arrival → P3 then P4), then P2(1)\n",
    "   ```\n",
    "   | P1 | P5 | P3 | P4  | P2 |\n",
    "   0    5    9   10    17   20\n",
    "   ```\n",
    "\n",
    "4. **RR (Round Robin, Quantum = 2)**\n",
    "   - All processes arrive at time 0, order: P1, P2, P3, P4, P5\n",
    "   ```\n",
    "   |P1|P2|P3|P4|P5|P1|P2|P4|P5|P1|P4|P5|P4|\n",
    "   0 2 4 5 6 8 10 11 13 14 16 18 19 20\n",
    "   ```\n",
    "   **Detailed RR Sequence:**\n",
    "   - Time 0-2: P1 (3 left)\n",
    "   - 2-4: P2 (1 left)\n",
    "   - 4-5: P3 (1 burst, finishes at 5)\n",
    "   - 5-6: P4 (5 left)\n",
    "   - 6-8: P5 (2 left)\n",
    "   - 8-10: P1 (1 left)\n",
    "   - 10-11: P2 (finishes at 11)\n",
    "   - 11-13: P4 (3 left)\n",
    "   - 13-14: P5 (finishes at 14)\n",
    "   - 14-16: P1 (finishes at 16)\n",
    "   - 16-18: P4 (1 left)\n",
    "   - 18-19: P4 (finishes at 19)\n",
    "   - 19-20: [CPU idle? Actually P4 finished at 19, so idle from 19-20]\n",
    "\n",
    "#### **b. Turnaround Time (TAT) Table**\n",
    "*TAT = Completion Time - Arrival Time (Arrival=0 for all)*\n",
    "\n",
    "| Process | FCFS | SJF | Priority | RR |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| P1 | 5 | 13 | 5 | 16 |\n",
    "| P2 | 8 | 4 | 20 | 11 |\n",
    "| P3 | 9 | 1 | 10 | 5 |\n",
    "| P4 | 16 | 20 | 17 | 19 |\n",
    "| P5 | 20 | 8 | 9 | 14 |\n",
    "\n",
    "#### **c. Waiting Time (WT) Table**\n",
    "*WT = Turnaround Time - Burst Time*\n",
    "\n",
    "| Process | Burst | FCFS | SJF | Priority | RR |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| P1 | 5 | 0 | 8 | 0 | 11 |\n",
    "| P2 | 3 | 5 | 1 | 17 | 8 |\n",
    "| P3 | 1 | 8 | 0 | 9 | 4 |\n",
    "| P4 | 7 | 9 | 13 | 10 | 12 |\n",
    "| P5 | 4 | 16 | 4 | 5 | 10 |\n",
    "\n",
    "#### **d. Algorithm with Minimum Average Waiting Time**\n",
    "Calculate averages from part (c):\n",
    "\n",
    "- **FCFS:** (0+5+8+9+16)/5 = 38/5 = **7.6**\n",
    "- **SJF:** (8+1+0+13+4)/5 = 26/5 = **5.2**\n",
    "- **Priority:** (0+17+9+10+5)/5 = 41/5 = **8.2**\n",
    "- **RR:** (11+8+4+12+10)/5 = 45/5 = **9.0**\n",
    "\n",
    "**SJF** gives the **minimum average waiting time (5.2)**, which is expected as SJF is optimal for minimizing average waiting time when all processes arrive simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.19: The `nice` Command and Privileges**\n",
    "\n",
    "### **Question**\n",
    "The `nice` command sets the nice value of a process on Linux/UNIX. Why do some systems allow any user to set nice value ≥ 0, but only root can set values < 0?\n",
    "\n",
    "### **Answer**\n",
    "This design prevents **priority inversion attacks** and ensures system stability by preventing non-privileged users from monopolizing CPU resources.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "In UNIX/Linux, the **nice value** ranges from -20 (highest priority) to +19 (lowest priority). The actual priority is calculated as: `Priority = Base + nice`, where lower numerical priority value means higher scheduling priority in the kernel.\n",
    "\n",
    "| Action | Effect | Privilege Required | Rationale |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Increase nice value (≥ 0)** | Lowers process priority (makes it \"nicer\" to others) | Any user | This is **safe**—it only reduces a process's share of CPU, potentially slowing it down but not harming other processes or system stability. |\n",
    "| **Decrease nice value (< 0)** | Raises process priority (makes it more aggressive) | Only root (superuser) | This is **dangerous**—a user could make their process higher priority than system daemons or other users' processes, causing **starvation** or system unresponsiveness. |\n",
    "\n",
    "**Security & Fairness Implications:**\n",
    "1. **Denial-of-Service Prevention:** If any user could set high priority (negative nice), they could monopolize CPU and starve critical system processes.\n",
    "2. **System Integrity:** Kernel and administrative processes need guaranteed CPU access for system maintenance, I/O handling, and security monitoring.\n",
    "3. **Fair Sharing:** In multi-user systems, the root administrator must arbitrate resource contention to ensure fair allocation among users.\n",
    "\n",
    "This privilege separation is a classic example of the **principle of least privilege** in OS design.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.20: Scheduling Algorithms and Starvation**\n",
    "\n",
    "### **Question**\n",
    "Which of the following scheduling algorithms could result in starvation?\n",
    "a. First-come, first-served\n",
    "b. Shortest job first\n",
    "c. Round robin\n",
    "d. Priority\n",
    "\n",
    "### **Answer**\n",
    "Algorithms that could result in starvation: **b. Shortest job first** and **d. Priority**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "**Starvation** occurs when a process is indefinitely denied CPU time because other processes are always preferred.\n",
    "\n",
    "| Algorithm | Starvation Possible? | Why |\n",
    "| :--- | :--- | :--- |\n",
    "| **FCFS** | **No** | Every process eventually gets the CPU in arrival order. No process is skipped indefinitely. |\n",
    "| **SJF** | **Yes** | A long job may wait forever if shorter jobs keep arriving. This is the classic starvation scenario for SJF. |\n",
    "| **Round Robin** | **No** | Each process gets a time slice periodically. No process waits more than (n-1)*q time units for n processes with quantum q. |\n",
    "| **Priority** | **Yes** | Low-priority processes may never run if higher-priority processes continuously arrive. This is common in pure priority scheduling. |\n",
    "\n",
    "**Mitigations:**\n",
    "- **Aging** is often used with SJF and Priority schedulers: gradually increasing priority of waiting processes to eventually break starvation.\n",
    "- **Multilevel feedback queues** combine RR at different priority levels with process migration between levels to prevent indefinite starvation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497f8f4",
   "metadata": {},
   "source": [
    "\n",
    "## **Exercise 5.21: Variant of RR with Pointers to PCBs**\n",
    "\n",
    "### **Question**\n",
    "Consider a variant of the RR scheduling algorithm in which the entries in the ready queue are pointers to the PCBs.\n",
    "\n",
    "a. What would be the effect of putting two pointers to the same process in the ready queue?\n",
    "b. What would be two major advantages and two disadvantages of this scheme?\n",
    "c. How would you modify the basic RR algorithm to achieve the same effect without the duplicate pointers?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Effect of Duplicate Pointers**\n",
    "Placing two pointers to the same process in the ready queue would cause that process to be **selected twice as often** in the round-robin cycle. It would receive approximately twice the CPU time compared to processes with only one pointer, effectively increasing its priority or weight.\n",
    "\n",
    "#### **b. Advantages and Disadvantages**\n",
    "**Advantages:**\n",
    "1. **Simple Priority Mechanism:** Provides an easy way to implement **weighted fair sharing** without complex priority scheduling algorithms.\n",
    "2. **Flexibility:** The scheduler can dynamically adjust the relative CPU allocation by adding or removing pointers.\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Starvation Risk:** If some processes have many pointers and others few, it can lead to unfairness or starvation.\n",
    "2. **Management Overhead:** When a process blocks or terminates, all its pointers must be located and removed from the queue, requiring additional operations.\n",
    "\n",
    "#### **c. Modification without Duplicate Pointers**\n",
    "To achieve proportional CPU allocation without duplicate pointers, modify RR to use **variable time quanta** or **weighted round-robin**:\n",
    "- Assign each process a **weight** (e.g., 2 for processes that would have had two pointers).\n",
    "- Allocate CPU time proportional to weights, either by adjusting time quantum sizes or by scheduling processes with frequency proportional to their weights.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.22: CPU Utilization with Mixed Workload**\n",
    "\n",
    "### **Question**\n",
    "Consider a system running ten I/O-bound tasks and one CPU-bound task. Assume that the I/O-bound tasks issue an I/O operation once for every millisecond of CPU computing and that each I/O operation takes 10 milliseconds to complete. Also assume that the context-switching overhead is 0.1 millisecond and that all processes are long-running tasks. Describe the CPU utilization for a round-robin scheduler when:\n",
    "\n",
    "a. The time quantum is 1 millisecond\n",
    "b. The time quantum is 10 milliseconds\n",
    "\n",
    "### **Answer**\n",
    "We define **CPU utilization** as the percentage of time the CPU is executing user processes (excluding context-switch overhead).\n",
    "\n",
    "**Given:**\n",
    "- 10 I/O-bound tasks: CPU burst = 1 ms, I/O time = 10 ms.\n",
    "- 1 CPU-bound task: CPU burst = entire quantum (never blocks).\n",
    "- Context switch time = 0.1 ms.\n",
    "- All tasks are long-running (always have work to do).\n",
    "\n",
    "#### **a. Time Quantum = 1 ms**\n",
    "Each I/O-bound task runs for 1 ms, then blocks for I/O. The CPU-bound task runs for 1 ms per quantum.\n",
    "\n",
    "**Cycle Analysis:**\n",
    "In a cycle where all 11 tasks run once:\n",
    "- Total CPU useful time = 10 × 1 ms (I/O tasks) + 1 × 1 ms (CPU task) = 11 ms.\n",
    "- Number of context switches = 11 (after each task).\n",
    "- Total context switch time = 11 × 0.1 ms = 1.1 ms.\n",
    "- Total cycle time = 11 ms + 1.1 ms = 12.1 ms.\n",
    "\n",
    "\\[\n",
    "\\text{CPU utilization} = \\frac{\\text{Useful CPU time}}{\\text{Total time}} = \\frac{11}{12.1} \\approx 0.909 = 90.9\\%\n",
    "\\]\n",
    "\n",
    "#### **b. Time Quantum = 10 ms**\n",
    "I/O-bound tasks still run for only 1 ms then block (they don't use full quantum). CPU-bound task runs for up to 10 ms.\n",
    "\n",
    "**Cycle Analysis:**\n",
    "Consider a period where all I/O tasks run once and the CPU task runs once:\n",
    "- Useful CPU time = 10 × 1 ms + 10 ms = 20 ms.\n",
    "- Number of context switches = 11 (10 after I/O tasks block, 1 after CPU task's quantum expires).\n",
    "- Total context switch time = 11 × 0.1 ms = 1.1 ms.\n",
    "- Total time = 20 ms + 1.1 ms = 21.1 ms.\n",
    "\n",
    "\\[\n",
    "\\text{CPU utilization} = \\frac{20}{21.1} \\approx 0.948 = 94.8\\%\n",
    "\\]\n",
    "\n",
    "**Conclusion:** A larger time quantum reduces context switch overhead and improves CPU utilization for this mixed workload.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.23: Maximizing CPU Time in Multilevel Queue Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider a system implementing multilevel queue scheduling. What strategy can a computer user employ to maximize the amount of CPU time allocated to the user’s process?\n",
    "\n",
    "### **Answer**\n",
    "A user can structure their process to **mimic an interactive (I/O-bound) process** to be placed in a higher-priority queue.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "Multilevel queue schedulers typically have:\n",
    "- **High-priority queues** for interactive processes (short time quanta, frequent scheduling).\n",
    "- **Low-priority queues** for batch/CPU-bound processes (long time quanta, infrequent scheduling).\n",
    "\n",
    "**Strategies:**\n",
    "1. **Frequent I/O Operations:** Insert periodic I/O calls (e.g., small writes, sleep calls) to make the process appear I/O-bound, favoring placement in a higher-priority queue.\n",
    "2. **Exploit Priority Rules:** If the system allows user-set priorities (e.g., `nice` values), set the process to highest allowed priority.\n",
    "3. **Process Characteristics:** Design the program to have short CPU bursts (< time quantum) to avoid being demoted to lower-priority queues in a **multilevel feedback queue** system.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.24: Dynamic Priority Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider a preemptive priority scheduling algorithm based on dynamically changing priorities. Larger priority numbers imply higher priority. When a process is waiting for the CPU (in the ready queue, but not running), its priority changes at a rate α. When it is running, its priority changes at a rate β. All processes are given a priority of 0 when they enter the ready queue. The parameters α and β can be set to give many different scheduling algorithms.\n",
    "\n",
    "a. What is the algorithm that results from β > α > 0?\n",
    "b. What is the algorithm that results from α < β < 0?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. β > α > 0 (Both positive, β larger)**\n",
    "- Running processes' priorities increase faster than waiting processes'.\n",
    "- Once a process starts running, it quickly becomes the highest priority and tends to keep the CPU.\n",
    "- This approximates **First-Come, First-Served (FCFS)** or **nonpreemptive scheduling**, as the initial process to run will retain control.\n",
    "\n",
    "#### **b. α < β < 0 (Both negative, α more negative)**\n",
    "- Priorities decrease (since negative rates), but waiting processes' priorities drop faster.\n",
    "- A running process retains a relatively higher priority compared to waiting processes.\n",
    "- New processes (priority 0) have higher priority than older waiting processes (negative priorities), leading to **Last-Come, First-Served (LCFS)** behavior, possibly with starvation of older processes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.25: Discrimination for Short Processes**\n",
    "\n",
    "### **Question**\n",
    "Explain how the following scheduling algorithms discriminate either in favor of or against short processes:\n",
    "\n",
    "a. FCFS\n",
    "b. RR\n",
    "c. Multilevel feedback queues\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. FCFS (First-Come, First-Served)**\n",
    "- **Discriminates against short processes.**\n",
    "- **Why:** The **convoy effect**—a short process arriving after a long process must wait for the long process to complete, increasing its waiting time disproportionately.\n",
    "\n",
    "#### **b. RR (Round Robin)**\n",
    "- **Treats all processes equally, but short processes benefit.**\n",
    "- **Why:** Each process gets equal time slices; short processes finish earlier because they require fewer total slices, reducing their turnaround time.\n",
    "\n",
    "#### **c. Multilevel Feedback Queues (MFQ)**\n",
    "- **Generally favors short (I/O-bound) processes.**\n",
    "- **Why:** Processes with short CPU bursts are promoted to higher-priority queues, getting more frequent CPU access. Long CPU-bound processes are demoted to lower-priority queues.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.26: Shared Ready Queue in SMP**\n",
    "\n",
    "### **Question**\n",
    "Describe why a shared ready queue might suffer from performance problems in an SMP environment.\n",
    "\n",
    "### **Answer**\n",
    "A shared ready queue becomes a **contention bottleneck** due to:\n",
    "1. **Lock Synchronization:** Multiple processors must lock the queue to enqueue/dequeue processes, causing serialization and spin-waiting.\n",
    "2. **Cache Coherence Overhead:** Frequent updates to the shared queue cause cache invalidation across processors, increasing memory latency.\n",
    "3. **Scalability Limit:** As processor count grows, contention increases, reducing parallel efficiency.\n",
    "\n",
    "**Solution:** Use per-processor run queues with work-stealing or load-balancing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.27: Load Balancing with Priority-Based Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider a load-balancing algorithm that ensures that each queue has approximately the same number of threads, independent of priority. How effectively would a priority-based scheduling algorithm handle this situation if one run queue had all high-priority threads and a second queue had all low-priority threads?\n",
    "\n",
    "### **Answer**\n",
    "It would handle it **poorly**. The load balancer ignores priority, leading to:\n",
    "- **Processor 1 (high-priority queue):** Responsive for high-priority threads.\n",
    "- **Processor 2 (low-priority queue):** Busy with low-priority threads, while high-priority threads might be waiting if Processor 1 is overloaded.\n",
    "- **Inefficiency:** Processor 2 could be executing low-priority threads while high-priority threads exist, violating priority-based scheduling goals.\n",
    "\n",
    "**Solution:** Load balancing should consider both thread count and priority, distributing high-priority threads across processors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.28: Process Placement in Per-Processor Run Queues**\n",
    "\n",
    "### **Question**\n",
    "Assume that an SMP system has private, per-processor run queues. When a new process is created, it can be placed in either the same queue as the parent process or a separate queue.\n",
    "\n",
    "a. What are the benefits of placing the new process in the same queue as its parent?\n",
    "b. What are the benefits of placing the new process in a different queue?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Same Queue as Parent**\n",
    "- **Cache Affinity:** Parent and child likely share code/data; keeping them on the same processor improves cache hit rates.\n",
    "- **Synchronization Efficiency:** If they communicate frequently, shared memory accesses are faster within the same processor’s cache.\n",
    "\n",
    "#### **b. Different Queue**\n",
    "- **Load Distribution:** Spreads work across processors, improving overall throughput.\n",
    "- **Parallel Execution:** Allows true concurrency if parent and child are independent.\n",
    "- **Fault Isolation:** A busy processor doesn’t delay both processes.\n",
    "\n",
    "**Trade-off:** Systems often use **affinity scheduling** (same queue initially) with **migration** for load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b466c6a",
   "metadata": {},
   "source": [
    "\n",
    "## **Exercise 5.29: NUMA-Aware Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Assume that a thread has blocked for network I/O and is eligible to run again. Describe why a NUMA-aware scheduling algorithm should reschedule the thread on the same CPU on which it previously ran.\n",
    "\n",
    "### **Answer**\n",
    "In a **NUMA (Non-Uniform Memory Access)** system, memory access time depends on the memory location relative to the processor. Each processor has **local memory** (fast access) and **remote memory** (slower access across interconnects).\n",
    "\n",
    "**Reasons for Rescheduling on Same CPU:**\n",
    "\n",
    "1. **Memory Locality:** The thread's memory pages were likely allocated in the **local memory** of the CPU it was running on. Rescheduling it on the same CPU ensures fast access to its working set.\n",
    "2. **Cache Warmth:** The CPU's caches may still contain the thread's data and instructions, reducing cache miss penalties.\n",
    "3. **Reduced NUMA Penalty:** If scheduled on a different CPU, the thread would experience:\n",
    "   - **Remote memory accesses** (3-5x slower than local)\n",
    "   - **Complete cache cold start** (all cache misses)\n",
    "   - **Increased memory contention** on remote memory controllers\n",
    "\n",
    "**Performance Impact:** In NUMA systems, incorrect thread placement can degrade performance by 30-50% due to remote memory access latency.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.30: Windows Thread Priority Calculation**\n",
    "\n",
    "### **Question**\n",
    "Using the Windows scheduling algorithm, determine the numeric priority of each of the following threads.\n",
    "\n",
    "Windows uses:\n",
    "- **Priority Classes** (base priorities): IDLE (4), BELOW_NORMAL (6), NORMAL (8), ABOVE_NORMAL (10), HIGH (13), REALTIME (24)\n",
    "- **Relative Priorities** within class: IDLE (-2), BELOW_NORMAL (-1), NORMAL (0), ABOVE_NORMAL (+1), HIGHEST (+2), TIME_CRITICAL (+3 for non-realtime, +16 for realtime)\n",
    "\n",
    "**Numeric Priority = Base(Priority Class) + Relative Priority**\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. REALTIME PRIORITY CLASS with NORMAL relative priority**\n",
    "- REALTIME base = 24\n",
    "- NORMAL relative = 0\n",
    "- **Numeric Priority = 24 + 0 = 24**\n",
    "\n",
    "#### **b. ABOVE NORMAL PRIORITY CLASS with HIGHEST relative priority**\n",
    "- ABOVE_NORMAL base = 10\n",
    "- HIGHEST relative = +2\n",
    "- **Numeric Priority = 10 + 2 = 12**\n",
    "\n",
    "#### **c. BELOW NORMAL PRIORITY CLASS with ABOVE NORMAL relative priority**\n",
    "- BELOW_NORMAL base = 6\n",
    "- ABOVE_NORMAL relative = +1\n",
    "- **Numeric Priority = 6 + 1 = 7**\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.31: Highest Possible Windows Priority**\n",
    "\n",
    "### **Question**\n",
    "Assuming that no threads belong to the REALTIME PRIORITY CLASS and that none may be assigned a TIME_CRITICAL priority, what combination of priority class and priority corresponds to the highest possible relative priority in Windows scheduling?\n",
    "\n",
    "### **Answer**\n",
    "**HIGH PRIORITY CLASS with HIGHEST relative priority**\n",
    "\n",
    "**Calculation:**\n",
    "- Without REALTIME class, the highest base priority is **HIGH** = 13\n",
    "- Without TIME_CRITICAL, the highest relative priority is **HIGHEST** = +2\n",
    "- **Numeric Priority = 13 + 2 = 15**\n",
    "\n",
    "**Alternative expression:** Threads in the HIGH priority class with HIGHEST relative adjustment have the highest possible priority under these constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.32: Solaris Time-Sharing Thread Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider the scheduling algorithm in Solaris for time-sharing threads.\n",
    "\n",
    "**Solaris Priority System:**\n",
    "- Priorities 0-59, higher number = higher priority\n",
    "- Time quantum = (priority ≤ 59) ? (20 + (priority/4)) ms : 200 ms\n",
    "- Priority adjustments:\n",
    "  - Uses entire quantum without blocking: priority decreases (penalized for being CPU-bound)\n",
    "  - Blocks before quantum expires: priority increases (rewarded for being I/O-bound)\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Time quantum for priority 15? Priority 40?**\n",
    "- **Priority 15:** Quantum = 20 + ⌊15/4⌋ = 20 + 3 = **23 ms**\n",
    "- **Priority 40:** Quantum = 20 + ⌊40/4⌋ = 20 + 10 = **30 ms**\n",
    "\n",
    "#### **b. Thread with priority 50 uses entire quantum without blocking**\n",
    "- Using entire quantum → **priority decreases**\n",
    "- Typical decrease: by 1-5 priority points (implementation specific)\n",
    "- **New priority likely in range 45-49** (probably 49 if decreasing by 1)\n",
    "\n",
    "#### **c. Thread with priority 20 blocks for I/O before quantum expires**\n",
    "- Blocking before quantum ends → **priority increases**\n",
    "- Typical increase: by 1-10 priority points (more if blocked early)\n",
    "- **New priority likely in range 21-30** (could be up to 30 for very early block)\n",
    "\n",
    "**Design Philosophy:** This implements a **multilevel feedback queue** that rewards I/O-bound (interactive) threads and penalizes CPU-bound threads.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.33: Linux CFS Scheduler and vruntime**\n",
    "\n",
    "### **Question**\n",
    "Assume two tasks, A (nice = -5) and B (nice = +5), running on Linux with CFS scheduler. Describe how their vruntime values vary in these scenarios:\n",
    "\n",
    "**CFS Basics:** \n",
    "- Each task has virtual runtime (vruntime) = actual runtime × weight factor\n",
    "- Lower nice = higher weight = vruntime increases slower\n",
    "- CFS always runs task with **lowest vruntime**\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **Scenario 1: Both A and B are CPU-bound**\n",
    "- Both constantly use CPU, accumulate actual runtime\n",
    "- A (nice = -5, higher priority) has **higher weight** → vruntime increases **slower** than B\n",
    "- B (nice = +5, lower priority) has **lower weight** → vruntime increases **faster**\n",
    "- **Result:** A's vruntime stays lower than B's, so A gets **more CPU time** (approximately 3:1 ratio due to weight differences)\n",
    "\n",
    "#### **Scenario 2: A is I/O-bound, B is CPU-bound**\n",
    "- A blocks frequently for I/O → accumulates little actual runtime → vruntime increases very slowly\n",
    "- B runs continuously → accumulates actual runtime quickly → vruntime increases rapidly\n",
    "- **Result:** A's vruntime stays very low, so when A becomes ready, it immediately preempts B. A gets excellent responsiveness.\n",
    "\n",
    "#### **Scenario 3: A is CPU-bound, B is I/O-bound**\n",
    "- A runs continuously → vruntime increases (but slower due to higher weight)\n",
    "- B blocks frequently → vruntime increases very slowly when it runs\n",
    "- **Result:** B's vruntime may become lower than A's when B wakes up, allowing B to preempt A. This ensures I/O-bound tasks remain responsive even with lower priority (nice +5).\n",
    "\n",
    "**Key Insight:** CFS achieves fairness by **comparing vruntime, not raw CPU time**, allowing it to properly handle mixed workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.34: Rate-Monotonic vs Earliest-Deadline-First**\n",
    "\n",
    "### **Question**\n",
    "Provide a specific circumstance that illustrates where rate-monotonic scheduling is inferior to earliest-deadline-first scheduling in meeting real-time process deadlines.\n",
    "\n",
    "### **Answer**\n",
    "**Scenario:**\n",
    "- **Task T1:** Period = 50 ms, Execution time = 25 ms, Deadline = 50 ms\n",
    "- **Task T2:** Period = 75 ms, Execution time = 30 ms, Deadline = 75 ms\n",
    "\n",
    "**Rate-Monotonic (RM) Analysis:**\n",
    "- Assigns higher priority to task with **shorter period** (T1)\n",
    "- T1 always preempts T2\n",
    "- Schedule T1 first (0-25), then T2 (25-55), but T1's next period starts at 50\n",
    "- **Problem:** At time 50, T1 preempts T2 (which started at 25 and needs until 55)\n",
    "- T1 runs 50-75, then T2 resumes 75-80 (but T2's deadline was 75!)\n",
    "- **Result:** T2 **misses deadline** at 75\n",
    "\n",
    "**Earliest-Deadline-First (EDF) Analysis:**\n",
    "- Always runs task with **earliest deadline**\n",
    "- Initially: T1 deadline = 50, T2 deadline = 75 → run T1 (0-25)\n",
    "- At 25: T1 next deadline = 50, T2 deadline = 75 → still run T1? Wait, need to check.\n",
    "- Actually, EDF would schedule: T1 (0-25), T2 (25-55), T1 (55-80)...\n",
    "- **Deadline check:** T2 completes at 55 < deadline 75 ✓, T1 completes at 80 < deadline 100 ✓\n",
    "\n",
    "**Conclusion:** RM fails this task set even though CPU utilization (25/50 + 30/75 = 0.5 + 0.4 = 0.9) is below the RM bound (~0.828 for 2 tasks). EDF successfully schedules it because EDF can achieve 100% utilization.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.35: Rate-Monotonic vs EDF Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider two processes:\n",
    "- P₁: period p₁ = 50, execution time t₁ = 25\n",
    "- P₂: period p₂ = 75, execution time t₂ = 30\n",
    "\n",
    "a. Can these be scheduled using rate-monotonic scheduling? Illustrate with Gantt chart.\n",
    "b. Illustrate scheduling using earliest-deadline-first (EDF).\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Rate-Monotonic Scheduling**\n",
    "**Priority:** P₁ has higher priority (shorter period = 50)\n",
    "\n",
    "**Gantt Chart (first 150 ms):**\n",
    "```\n",
    "Time:   0    25    50    75    100   125   150\n",
    "        |-----|-----|-----|-----|-----|-----|\n",
    "P1:     [====]     [====]     [====]     [====]\n",
    "        0-25        50-75      100-125\n",
    "P2:           [==============]           [====...\n",
    "                25-55 (preempted!)          125-155\n",
    "```\n",
    "**Detailed Breakdown:**\n",
    "- 0-25: P₁ runs (completes)\n",
    "- 25-50: P₂ runs (needs 30 total, gets 25 so far)\n",
    "- 50-55: P₁ preempts (higher priority), P₂ paused with 5 ms remaining\n",
    "- 55-75: P₁ completes its 25 ms burst\n",
    "- 75-80: P₂ resumes and completes its remaining 5 ms\n",
    "- But P₂'s first deadline was at 75! It completed at 80 → **MISSED DEADLINE**\n",
    "\n",
    "**Conclusion:** RM fails. Utilization = 25/50 + 30/75 = 0.5 + 0.4 = 0.9 > 0.828 (RM bound for 2 tasks), so guaranteed to fail.\n",
    "\n",
    "#### **b. Earliest-Deadline-First Scheduling**\n",
    "EDF always runs task with earliest absolute deadline.\n",
    "\n",
    "**Gantt Chart (first 150 ms):**\n",
    "```\n",
    "Time:   0    25    50    75    100   125   150\n",
    "        |-----|-----|-----|-----|-----|-----|\n",
    "P1:     [====]           [====]     [====]\n",
    "        0-25 (d=50)      55-80     105-130\n",
    "                         (d=100)    (d=150)\n",
    "P2:           [==============]     [======...\n",
    "                25-55               80-110\n",
    "                (d=75)              (d=150)\n",
    "```\n",
    "**Schedule:**\n",
    "- 0-25: P₁ runs (deadline 50)\n",
    "- 25-55: P₂ runs (deadline 75) - completes at 55 < 75 ✓\n",
    "- 55-80: P₁ runs (next instance, deadline 100) - completes at 80 < 100 ✓\n",
    "- 80-110: P₂ runs (next instance, deadline 150) - completes at 110 < 150 ✓\n",
    "- 105-130: P₁ runs (skipped? Actually P₁ at 100... Let's recalculate carefully)\n",
    "\n",
    "**Correct EDF Schedule:**\n",
    "Time 0: P₁(d50), P₂(d75) → run P₁ (0-25)\n",
    "Time 25: P₁ next d=100, P₂(d75) → run P₂ (25-55)\n",
    "Time 55: P₁(d100), P₂ next d=150 → run P₁ (55-80)\n",
    "Time 80: P₁ next d=150, P₂(d150) → tie, can run either (say P₂: 80-110)\n",
    "Time 110: P₁(d150) → run P₁ (110-135)\n",
    "All deadlines met ✓\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.36: Bounded Latency in Hard Real-Time Systems**\n",
    "\n",
    "### **Question**\n",
    "Explain why interrupt and dispatch latency times must be bounded in a hard real-time system.\n",
    "\n",
    "### **Answer**\n",
    "In **hard real-time systems**, missing a deadline can cause **catastrophic failure** (e.g., aircraft control, medical devices, nuclear plant control).\n",
    "\n",
    "**Two Critical Latencies:**\n",
    "\n",
    "1. **Interrupt Latency:** Time from interrupt signal to start of ISR\n",
    "   - Must be bounded to ensure timely response to external events\n",
    "   - Factors: OS may disable interrupts (critical sections), interrupt controller delays\n",
    "\n",
    "2. **Dispatch Latency:** Time from ISR completion to task start\n",
    "   - Must be bounded to ensure high-priority tasks run promptly\n",
    "   - Factors: Context switch time, scheduler decisions, cache flushing\n",
    "\n",
    "**Why Bounded/Boundable:**\n",
    "- **Predictability:** System must guarantee worst-case response time ≤ deadline\n",
    "- **Schedulability Analysis:** Mathematical analysis (e.g., RM, EDF) requires known maximum latencies\n",
    "- **Certification:** Safety-critical systems require formal verification of timing constraints\n",
    "\n",
    "**Example:** In fly-by-wire systems, control loop must complete within 1-2 ms. Unbounded latency could cause oscillation or loss of control.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.37: Heterogeneous Multiprocessing in Mobile Systems**\n",
    "\n",
    "### **Question**\n",
    "Describe the advantages of using heterogeneous multiprocessing in a mobile system.\n",
    "\n",
    "### **Answer**\n",
    "**Heterogeneous Multiprocessing (HMP)** uses cores with different performance/power characteristics (e.g., ARM big.LITTLE: few \"big\" high-performance cores + many \"LITTLE\" power-efficient cores).\n",
    "\n",
    "### **Advantages:**\n",
    "\n",
    "1. **Energy Efficiency:** \n",
    "   - Light tasks → LITTLE cores (low power)\n",
    "   - Heavy tasks → big cores (high performance)\n",
    "   - Dynamic migration balances performance needs with battery life\n",
    "\n",
    "2. **Thermal Management:**\n",
    "   - Spread heat generation across different core types\n",
    "   - Use LITTLE cores when thermal throttling required\n",
    "   - Prevents overheating and performance degradation\n",
    "\n",
    "3. **Performance Scaling:**\n",
    "   - Fine-grained control over performance/power trade-off\n",
    "   - Better than DVFS alone (dynamic voltage/frequency scaling)\n",
    "\n",
    "4. **Cost Optimization:**\n",
    "   - Mix of core types provides better performance/$ than homogeneous design\n",
    "   - Smaller LITTLE cores occupy less die area\n",
    "\n",
    "5. **Workload Matching:**\n",
    "   - Background tasks (email sync, notifications) → LITTLE cores\n",
    "   - Foreground tasks (gaming, camera) → big cores\n",
    "   - OS scheduler intelligently places threads based on requirements\n",
    "\n",
    "**Example:** Samsung Exynos/Qualcomm Snapdragon use HMP to achieve all-day battery life while providing burst performance when needed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
