{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997accf8",
   "metadata": {},
   "source": [
    "# Handbook of Solutions: Chapter 5 Exercises (Abraham-Silberschatz OS Concepts 10th Ed.)\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.11: Context Switches for I/O-Bound vs. CPU-Bound Programs**\n",
    "\n",
    "### **Question**\n",
    "Of these two types of programs:\n",
    "a. I/O-bound\n",
    "b. CPU-bound\n",
    "\n",
    "which is more likely to have voluntary context switches, and which is more likely to have nonvoluntary context switches? Explain your answer.\n",
    "\n",
    "### **Answer**\n",
    "*   **I/O-bound programs** are more likely to have **voluntary context switches**.\n",
    "*   **CPU-bound programs** are more likely to have **nonvoluntary context switches**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "A **context switch** is the process of saving the state of one process and loading the saved state of another. They are categorized by their cause:\n",
    "\n",
    "| Type | Cause | How it Happens |\n",
    "| :--- | :--- | :--- |\n",
    "| **Voluntary** | The running process **gives up the CPU willingly**. | Process issues a system call (e.g., for I/O, `wait()`, `yield()`). |\n",
    "| **Nonvoluntary (Preemptive)** | The OS **forcibly removes the process** from the CPU. | Interrupt occurs (timer expiration, higher-priority process arrival). |\n",
    "\n",
    "**I/O-Bound Programs:**\n",
    "*   **Characteristic:** Frequent, short CPU bursts followed by I/O operations (e.g., reading from disk, waiting for user input).\n",
    "*   **Behavior:** After a short CPU burst, the program will issue an I/O request (like `read()`). This system call triggers a transition to the **waiting** state, causing a **voluntary context switch**.\n",
    "*   **Conclusion:** Their execution pattern naturally leads to many voluntary context switches.\n",
    "\n",
    "**CPU-Bound Programs:**\n",
    "*   **Characteristic:** Long CPU bursts with infrequent I/O operations (e.g., scientific calculations, matrix multiplication).\n",
    "*   **Behavior:** They rarely issue I/O requests and thus rarely give up the CPU voluntarily. To prevent them from monopolizing the CPU and ensure fairness, the OS relies on **timer interrupts**. When the time quantum (in RR) expires or a higher-priority process arrives, the OS performs a **nonvoluntary context switch**.\n",
    "*   **Conclusion:** They are primarily preempted by the OS scheduler, leading to nonvoluntary context switches.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.12: Conflicting Scheduling Criteria**\n",
    "\n",
    "### **Question**\n",
    "Discuss how the following pairs of scheduling criteria conflict in certain settings.\n",
    "\n",
    "### **Answer & Explanation**\n",
    "\n",
    "Scheduling algorithm design involves trade-offs. Optimizing for one metric often degrades another.\n",
    "\n",
    "#### **a. CPU Utilization and Response Time**\n",
    "*   **Conflict:** Maximizing CPU utilization can degrade response time, and vice-versa.\n",
    "*   **Explanation:**\n",
    "    *   **High CPU Utilization** is achieved by keeping the CPU busy at all times, often by favoring **CPU-bound processes** with long bursts or by minimizing context switches.\n",
    "    *   **Good Response Time** (important for interactive systems) requires the CPU to be frequently available to handle short, incoming requests. This often means **preempting** long-running CPU-bound tasks to service interactive tasks, which introduces context switches and can leave the CPU idle briefly while switching.\n",
    "    *   **Example:** In a **batch processing system** (goal: high utilization), using FCFS might keep the CPU 100% busy but lead to terrible response times for short interactive tasks. In a **time-sharing system** (goal: good response time), using RR with a small time quantum improves responsiveness but increases context switch overhead, potentially lowering overall CPU utilization.\n",
    "\n",
    "#### **b. Average Turnaround Time and Maximum Waiting Time**\n",
    "*   **Conflict:** Minimizing the average turnaround time can lead to an unacceptably high maximum waiting time for some processes (starvation).\n",
    "*   **Explanation:**\n",
    "    *   **Minimizing Average Turnaround Time** is the goal of algorithms like **SJF/SRTF**. They prioritize short jobs, which reduces the average.\n",
    "    *   However, this can cause **indefinite postponement (starvation)** of long jobs. If short jobs keep arriving, a long job may wait indefinitely, causing its waiting time—and the system's **maximum waiting time**—to grow unbounded.\n",
    "    *   **Example:** In a pure nonpreemptive SJF schedule, a single very long job will always be passed over if shorter jobs continue to arrive, maximizing its individual waiting time.\n",
    "\n",
    "#### **c. I/O Device Utilization and CPU Utilization**\n",
    "*   **Conflict:** Over-optimizing for one can lead to under-utilization of the other.\n",
    "*   **Explanation:**\n",
    "    *   **High CPU Utilization** is achieved by running CPU-bound tasks that rarely perform I/O.\n",
    "    *   **High I/O Device Utilization** is achieved by keeping I/O devices busy, which requires having multiple I/O-bound processes that frequently issue I/O requests.\n",
    "    *   The conflict arises in finding a balance. A system full of CPU-bound processes will have high CPU utilization but low I/O utilization (devices sit idle). A system full of I/O-bound processes will keep devices busy but may leave the CPU underutilized because processes are often blocked waiting for I/O.\n",
    "    *   The ideal is a **mixed workload** where when one process is using the CPU, another is using an I/O device, allowing both to be utilized concurrently. However, scheduling to achieve perfect overlap is complex and these metrics can directly conflict if the workload is unbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.13: Implementing Priority in Lottery Scheduling**\n",
    "\n",
    "### **Question**\n",
    "In lottery scheduling, processes hold lottery tickets for CPU allocation. The BTV OS holds 50 lotteries/second, with each winner getting 20 ms. Describe how the BTV scheduler can ensure that higher-priority threads receive more attention from the CPU than lower-priority threads.\n",
    "\n",
    "### **Answer**\n",
    "The scheduler can assign tickets to threads **in proportion to their priority**. Higher-priority threads receive more tickets, increasing their probability of winning each lottery.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "Lottery scheduling is a **probabilistic** algorithm that provides proportional-share resource allocation. The key mechanism is the distribution of tickets.\n",
    "\n",
    "1.  **Assigning Tickets:** Each thread is assigned a number of lottery tickets. This number is not fixed; it is determined by the thread's **priority**. For example:\n",
    "    *   Priority 10 (High): 100 tickets\n",
    "    *   Priority 5 (Medium): 50 tickets\n",
    "    *   Priority 1 (Low): 10 tickets\n",
    "    The total tickets in the system = sum of all threads' tickets.\n",
    "\n",
    "2.  **Holding the Lottery:** When a scheduling decision is needed (50 times per second in BTV), the scheduler picks a random number corresponding to a ticket. The thread holding that winning ticket runs for the next 20 ms time slice.\n",
    "\n",
    "3.  **Mathematical Guarantee:** A thread's probability of winning the lottery (and thus receiving CPU time) is:\n",
    "    $$\n",
    "    P(\\text{thread}) = \\frac{\\text{Tickets held by thread}}{\\text{Total tickets in system}}\n",
    "    $$\n",
    "    Therefore, a thread with twice as many tickets as another has **twice the probability** of being scheduled, ensuring it receives \"more attention\" over time.\n",
    "\n",
    "4.  **Dynamic Adjustment:** The system can dynamically adjust ticket allocations based on changing priorities or to enforce fairness, providing a flexible mechanism for priority-based scheduling without the starvation risks of strict priority scheduling.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.14: Run Queue Design for Multicore Systems**\n",
    "\n",
    "### **Question**\n",
    "On multicore systems, there are two general options for the run queue: (1) per-core run queues, or (2) a single shared run queue. What are the advantages and disadvantages of each?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "| Approach | Advantages | Disadvantages |\n",
    "| :--- | :--- | :--- |\n",
    "| **Per-Core Run Queues** (Each core has its own private queue) | **1. Scalability:** No locking contention for the queue, allowing cores to schedule independently. <br> **2. Cache Locality:** A process tends to run on the same core, making better use of cached data. <br> **3. Reduced Overhead:** No need for complex locking mechanisms for queue access. | **1. Load Imbalance:** One core may be idle while another has a long queue. Requires a **load-balancing** mechanism. <br> **2. Complexity of Migration:** Moving a process from one core's queue to another's (for load balancing) is costly and can break cache locality. |\n",
    "| **Single Shared Run Queue** (One global queue for all cores) | **1. Automatic Load Balancing:** Cores automatically take the next available process, naturally balancing load. <br> **2. Simplicity:** Conceptually simple, easy to implement fairness policies (e.g., global RR). | **1. Scalability Bottleneck:** The single queue becomes a **contended shared resource**. Cores must lock the queue to access it, which can serialize scheduling decisions and hurt performance as core count increases. <br> **2. Poor Cache Locality:** A process may execute on a different core each time it is scheduled, causing **cache misses**. |\n",
    "\n",
    "**Conclusion:** Modern OSes typically use a **hybrid approach** (e.g., per-core queues with periodic load balancing) to mitigate the disadvantages of each pure design.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.15: Implications of Parameters in Exponential Averaging**\n",
    "\n",
    "### **Question**\n",
    "Consider the exponential average formula used to predict the length of the next CPU burst:\n",
    "$$\n",
    "\\tau_{n+1} = \\alpha t_n + (1 - \\alpha) \\tau_n\n",
    "$$\n",
    "Where:\n",
    "*   \\( \\tau_{n+1} \\) = predicted length of next CPU burst\n",
    "*   \\( t_n \\) = actual length of the \\(n\\)th CPU burst\n",
    "*   \\( \\tau_n \\) = previous prediction\n",
    "*   \\( \\alpha \\) = weighting parameter (\\(0 \\leq \\alpha \\leq 1\\))\n",
    "\n",
    "What are the implications of assigning the following values?\n",
    "\n",
    "### **Answer & Explanation**\n",
    "\n",
    "The parameter \\( \\alpha \\) controls the weight given to the **most recent observation** versus the **entire past history**.\n",
    "\n",
    "#### **a. α = 0 and τ₀ = 100 milliseconds**\n",
    "*   **Implication:** **The prediction never changes. History is everything; recent behavior is ignored.**\n",
    "*   **Explanation:**\n",
    "    *   Substituting \\( \\alpha = 0 \\) into the formula: \\( \\tau_{n+1} = 0 \\cdot t_n + (1-0) \\cdot \\tau_n = \\tau_n \\).\n",
    "    *   The new prediction is always equal to the old prediction, regardless of the actual burst length \\( t_n \\).\n",
    "    *   The system will **always use the initial guess \\( \\tau_0 = 100 \\)** for all future predictions. This eliminates the adaptive nature of the algorithm, making it useless for tracking changes in process behavior.\n",
    "\n",
    "#### **b. α = 0.99 and τ₀ = 10 milliseconds**\n",
    "*   **Implication:** **The prediction is overwhelmingly based on the most recent burst. Past history is nearly irrelevant.**\n",
    "*   **Explanation:**\n",
    "    *   Substituting \\( \\alpha = 0.99 \\): \\( \\tau_{n+1} = 0.99 \\cdot t_n + 0.01 \\cdot \\tau_n \\).\n",
    "    *   The new prediction is 99% determined by the actual length of the last CPU burst (\\( t_n \\)) and only 1% by the entire previous history (\\( \\tau_n \\)).\n",
    "    *   The algorithm becomes **extremely responsive to recent changes** but also highly volatile. A single anomalous long or short burst will drastically alter the prediction. The initial guess \\( \\tau_0 = 10 \\) becomes irrelevant after just one or two bursts.\n",
    "    *   This setting is useful if a process's burst lengths are **highly variable** and you want the predictor to quickly adapt to a new pattern. However, it may overreact to noise.\n",
    "\n",
    "**General Insight:** Choosing \\( \\alpha \\) is a trade-off between **stability** (low \\( \\alpha \\), smooth predictions) and **responsiveness** (high \\( \\alpha \\), quick adaptation). A typical value like \\( \\alpha = 0.5 \\) provides a balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f25dfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Exercise 5.16: Regressive Round-Robin Scheduler**\n",
    "\n",
    "### **Question**\n",
    "A variation of round-robin called **regressive round-robin** assigns each process a time quantum and priority. Initial time quantum = 50 ms. Rules:\n",
    "1. If a process uses its **entire quantum** (doesn't block for I/O): \n",
    "   - Add 10 ms to its quantum (max 100 ms)\n",
    "   - Boost its priority\n",
    "2. If a process **blocks before quantum ends**:\n",
    "   - Reduce quantum by 5 ms\n",
    "   - Priority unchanged\n",
    "\n",
    "Which process type (CPU-bound or I/O-bound) does this scheduler favor? Explain.\n",
    "\n",
    "### **Answer**\n",
    "This scheduler **favors I/O-bound processes**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "The scheduler's rules create feedback that rewards I/O-bound behavior and penalizes CPU-bound behavior:\n",
    "\n",
    "| Process Type | Typical Behavior | Effect under Regressive RR | Consequence |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **I/O-Bound** | Frequently blocks for I/O before quantum expires. | Quantum decreases by 5 ms each schedule, but **priority remains same**. | While quantum shrinks, the process maintains its priority level and actually gets scheduled more **frequently** (due to shorter bursts), leading to better responsiveness. |\n",
    "| **CPU-Bound** | Rarely blocks; uses full quantum. | Quantum increases (up to 100 ms), but **priority is boosted** (likely meaning lowered in Linux-style where higher number = lower priority). | Gets longer quanta but at **lower priority**. This means it will be scheduled less often than higher-priority I/O-bound processes. |\n",
    "\n",
    "**Key Insight:** The scheduler achieves **priority inversion** relative to standard RR:\n",
    "- **I/O-bound processes** keep their original (higher) priority and get shorter quanta → they finish their CPU work quickly and return to I/O, getting frequent, responsive service.\n",
    "- **CPU-bound processes** get lower priority and longer quanta → they run for longer periods but less frequently, preventing them from monopolizing the CPU.\n",
    "\n",
    "This design explicitly favors **interactive/I/O-bound processes** by ensuring they don't get stuck behind long CPU bursts, improving overall system responsiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.17: Comprehensive Scheduling Comparison (Second Set)**\n",
    "\n",
    "### **Question**\n",
    "Consider the following processes (all arriving at time 0):\n",
    "\n",
    "| Process | Burst Time | Priority |\n",
    "| :--- | :---: | :---: |\n",
    "| P1 | 5 | 4 |\n",
    "| P2 | 3 | 1 |\n",
    "| P3 | 1 | 2 |\n",
    "| P4 | 7 | 2 |\n",
    "| P5 | 4 | 3 |\n",
    "\n",
    "*(Higher priority number = higher priority)*\n",
    "\n",
    "**Tasks:**\n",
    "a. Draw Gantt charts for: FCFS, SJF, Nonpreemptive Priority, RR (quantum=2).\n",
    "b. Calculate turnaround time for each process under each algorithm.\n",
    "c. Calculate waiting time for each process.\n",
    "d. Identify which algorithm gives minimum average waiting time.\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Gantt Charts**\n",
    "\n",
    "1. **FCFS (First-Come, First-Served)**\n",
    "   ```\n",
    "   | P1 | P2 | P3 | P4  | P5   |\n",
    "   0    5    8    9     16    20\n",
    "   ```\n",
    "\n",
    "2. **SJF (Shortest-Job-First) - Nonpreemptive**\n",
    "   - Order by burst time: P3(1), P2(3), P5(4), P1(5), P4(7)\n",
    "   ```\n",
    "   | P3 | P2 | P5 | P1 | P4  |\n",
    "   0    1    4    8   13    20\n",
    "   ```\n",
    "\n",
    "3. **Nonpreemptive Priority** (Higher number = higher priority)\n",
    "   - Priorities: P1(4), P2(1), P3(2), P4(2), P5(3)\n",
    "   - Order by priority: P1(4), P5(3), then P3 & P4 (both 2, tie-break by arrival → P3 then P4), then P2(1)\n",
    "   ```\n",
    "   | P1 | P5 | P3 | P4  | P2 |\n",
    "   0    5    9   10    17   20\n",
    "   ```\n",
    "\n",
    "4. **RR (Round Robin, Quantum = 2)**\n",
    "   - All processes arrive at time 0, order: P1, P2, P3, P4, P5\n",
    "   ```\n",
    "   |P1|P2|P3|P4|P5|P1|P2|P4|P5|P1|P4|P5|P4|\n",
    "   0 2 4 5 6 8 10 11 13 14 16 18 19 20\n",
    "   ```\n",
    "   **Detailed RR Sequence:**\n",
    "   - Time 0-2: P1 (3 left)\n",
    "   - 2-4: P2 (1 left)\n",
    "   - 4-5: P3 (1 burst, finishes at 5)\n",
    "   - 5-6: P4 (5 left)\n",
    "   - 6-8: P5 (2 left)\n",
    "   - 8-10: P1 (1 left)\n",
    "   - 10-11: P2 (finishes at 11)\n",
    "   - 11-13: P4 (3 left)\n",
    "   - 13-14: P5 (finishes at 14)\n",
    "   - 14-16: P1 (finishes at 16)\n",
    "   - 16-18: P4 (1 left)\n",
    "   - 18-19: P4 (finishes at 19)\n",
    "   - 19-20: [CPU idle? Actually P4 finished at 19, so idle from 19-20]\n",
    "\n",
    "#### **b. Turnaround Time (TAT) Table**\n",
    "*TAT = Completion Time - Arrival Time (Arrival=0 for all)*\n",
    "\n",
    "| Process | FCFS | SJF | Priority | RR |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| P1 | 5 | 13 | 5 | 16 |\n",
    "| P2 | 8 | 4 | 20 | 11 |\n",
    "| P3 | 9 | 1 | 10 | 5 |\n",
    "| P4 | 16 | 20 | 17 | 19 |\n",
    "| P5 | 20 | 8 | 9 | 14 |\n",
    "\n",
    "#### **c. Waiting Time (WT) Table**\n",
    "*WT = Turnaround Time - Burst Time*\n",
    "\n",
    "| Process | Burst | FCFS | SJF | Priority | RR |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| P1 | 5 | 0 | 8 | 0 | 11 |\n",
    "| P2 | 3 | 5 | 1 | 17 | 8 |\n",
    "| P3 | 1 | 8 | 0 | 9 | 4 |\n",
    "| P4 | 7 | 9 | 13 | 10 | 12 |\n",
    "| P5 | 4 | 16 | 4 | 5 | 10 |\n",
    "\n",
    "#### **d. Algorithm with Minimum Average Waiting Time**\n",
    "Calculate averages from part (c):\n",
    "\n",
    "- **FCFS:** (0+5+8+9+16)/5 = 38/5 = **7.6**\n",
    "- **SJF:** (8+1+0+13+4)/5 = 26/5 = **5.2**\n",
    "- **Priority:** (0+17+9+10+5)/5 = 41/5 = **8.2**\n",
    "- **RR:** (11+8+4+12+10)/5 = 45/5 = **9.0**\n",
    "\n",
    "**SJF** gives the **minimum average waiting time (5.2)**, which is expected as SJF is optimal for minimizing average waiting time when all processes arrive simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.19: The `nice` Command and Privileges**\n",
    "\n",
    "### **Question**\n",
    "The `nice` command sets the nice value of a process on Linux/UNIX. Why do some systems allow any user to set nice value ≥ 0, but only root can set values < 0?\n",
    "\n",
    "### **Answer**\n",
    "This design prevents **priority inversion attacks** and ensures system stability by preventing non-privileged users from monopolizing CPU resources.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "In UNIX/Linux, the **nice value** ranges from -20 (highest priority) to +19 (lowest priority). The actual priority is calculated as: `Priority = Base + nice`, where lower numerical priority value means higher scheduling priority in the kernel.\n",
    "\n",
    "| Action | Effect | Privilege Required | Rationale |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Increase nice value (≥ 0)** | Lowers process priority (makes it \"nicer\" to others) | Any user | This is **safe**—it only reduces a process's share of CPU, potentially slowing it down but not harming other processes or system stability. |\n",
    "| **Decrease nice value (< 0)** | Raises process priority (makes it more aggressive) | Only root (superuser) | This is **dangerous**—a user could make their process higher priority than system daemons or other users' processes, causing **starvation** or system unresponsiveness. |\n",
    "\n",
    "**Security & Fairness Implications:**\n",
    "1. **Denial-of-Service Prevention:** If any user could set high priority (negative nice), they could monopolize CPU and starve critical system processes.\n",
    "2. **System Integrity:** Kernel and administrative processes need guaranteed CPU access for system maintenance, I/O handling, and security monitoring.\n",
    "3. **Fair Sharing:** In multi-user systems, the root administrator must arbitrate resource contention to ensure fair allocation among users.\n",
    "\n",
    "This privilege separation is a classic example of the **principle of least privilege** in OS design.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.20: Scheduling Algorithms and Starvation**\n",
    "\n",
    "### **Question**\n",
    "Which of the following scheduling algorithms could result in starvation?\n",
    "a. First-come, first-served\n",
    "b. Shortest job first\n",
    "c. Round robin\n",
    "d. Priority\n",
    "\n",
    "### **Answer**\n",
    "Algorithms that could result in starvation: **b. Shortest job first** and **d. Priority**.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "\n",
    "**Starvation** occurs when a process is indefinitely denied CPU time because other processes are always preferred.\n",
    "\n",
    "| Algorithm | Starvation Possible? | Why |\n",
    "| :--- | :--- | :--- |\n",
    "| **FCFS** | **No** | Every process eventually gets the CPU in arrival order. No process is skipped indefinitely. |\n",
    "| **SJF** | **Yes** | A long job may wait forever if shorter jobs keep arriving. This is the classic starvation scenario for SJF. |\n",
    "| **Round Robin** | **No** | Each process gets a time slice periodically. No process waits more than (n-1)*q time units for n processes with quantum q. |\n",
    "| **Priority** | **Yes** | Low-priority processes may never run if higher-priority processes continuously arrive. This is common in pure priority scheduling. |\n",
    "\n",
    "**Mitigations:**\n",
    "- **Aging** is often used with SJF and Priority schedulers: gradually increasing priority of waiting processes to eventually break starvation.\n",
    "- **Multilevel feedback queues** combine RR at different priority levels with process migration between levels to prevent indefinite starvation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497f8f4",
   "metadata": {},
   "source": [
    "\n",
    "## **Exercise 5.21: Variant of RR with Pointers to PCBs**\n",
    "\n",
    "### **Question**\n",
    "Consider a variant of the RR scheduling algorithm in which the entries in the ready queue are pointers to the PCBs.\n",
    "\n",
    "a. What would be the effect of putting two pointers to the same process in the ready queue?\n",
    "b. What would be two major advantages and two disadvantages of this scheme?\n",
    "c. How would you modify the basic RR algorithm to achieve the same effect without the duplicate pointers?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Effect of Duplicate Pointers**\n",
    "Placing two pointers to the same process in the ready queue would cause that process to be **selected twice as often** in the round-robin cycle. It would receive approximately twice the CPU time compared to processes with only one pointer, effectively increasing its priority or weight.\n",
    "\n",
    "#### **b. Advantages and Disadvantages**\n",
    "**Advantages:**\n",
    "1. **Simple Priority Mechanism:** Provides an easy way to implement **weighted fair sharing** without complex priority scheduling algorithms.\n",
    "2. **Flexibility:** The scheduler can dynamically adjust the relative CPU allocation by adding or removing pointers.\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Starvation Risk:** If some processes have many pointers and others few, it can lead to unfairness or starvation.\n",
    "2. **Management Overhead:** When a process blocks or terminates, all its pointers must be located and removed from the queue, requiring additional operations.\n",
    "\n",
    "#### **c. Modification without Duplicate Pointers**\n",
    "To achieve proportional CPU allocation without duplicate pointers, modify RR to use **variable time quanta** or **weighted round-robin**:\n",
    "- Assign each process a **weight** (e.g., 2 for processes that would have had two pointers).\n",
    "- Allocate CPU time proportional to weights, either by adjusting time quantum sizes or by scheduling processes with frequency proportional to their weights.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.22: CPU Utilization with Mixed Workload**\n",
    "\n",
    "### **Question**\n",
    "Consider a system running ten I/O-bound tasks and one CPU-bound task. Assume that the I/O-bound tasks issue an I/O operation once for every millisecond of CPU computing and that each I/O operation takes 10 milliseconds to complete. Also assume that the context-switching overhead is 0.1 millisecond and that all processes are long-running tasks. Describe the CPU utilization for a round-robin scheduler when:\n",
    "\n",
    "a. The time quantum is 1 millisecond\n",
    "b. The time quantum is 10 milliseconds\n",
    "\n",
    "### **Answer**\n",
    "We define **CPU utilization** as the percentage of time the CPU is executing user processes (excluding context-switch overhead).\n",
    "\n",
    "**Given:**\n",
    "- 10 I/O-bound tasks: CPU burst = 1 ms, I/O time = 10 ms.\n",
    "- 1 CPU-bound task: CPU burst = entire quantum (never blocks).\n",
    "- Context switch time = 0.1 ms.\n",
    "- All tasks are long-running (always have work to do).\n",
    "\n",
    "#### **a. Time Quantum = 1 ms**\n",
    "Each I/O-bound task runs for 1 ms, then blocks for I/O. The CPU-bound task runs for 1 ms per quantum.\n",
    "\n",
    "**Cycle Analysis:**\n",
    "In a cycle where all 11 tasks run once:\n",
    "- Total CPU useful time = 10 × 1 ms (I/O tasks) + 1 × 1 ms (CPU task) = 11 ms.\n",
    "- Number of context switches = 11 (after each task).\n",
    "- Total context switch time = 11 × 0.1 ms = 1.1 ms.\n",
    "- Total cycle time = 11 ms + 1.1 ms = 12.1 ms.\n",
    "\n",
    "\\[\n",
    "\\text{CPU utilization} = \\frac{\\text{Useful CPU time}}{\\text{Total time}} = \\frac{11}{12.1} \\approx 0.909 = 90.9\\%\n",
    "\\]\n",
    "\n",
    "#### **b. Time Quantum = 10 ms**\n",
    "I/O-bound tasks still run for only 1 ms then block (they don't use full quantum). CPU-bound task runs for up to 10 ms.\n",
    "\n",
    "**Cycle Analysis:**\n",
    "Consider a period where all I/O tasks run once and the CPU task runs once:\n",
    "- Useful CPU time = 10 × 1 ms + 10 ms = 20 ms.\n",
    "- Number of context switches = 11 (10 after I/O tasks block, 1 after CPU task's quantum expires).\n",
    "- Total context switch time = 11 × 0.1 ms = 1.1 ms.\n",
    "- Total time = 20 ms + 1.1 ms = 21.1 ms.\n",
    "\n",
    "\\[\n",
    "\\text{CPU utilization} = \\frac{20}{21.1} \\approx 0.948 = 94.8\\%\n",
    "\\]\n",
    "\n",
    "**Conclusion:** A larger time quantum reduces context switch overhead and improves CPU utilization for this mixed workload.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.23: Maximizing CPU Time in Multilevel Queue Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider a system implementing multilevel queue scheduling. What strategy can a computer user employ to maximize the amount of CPU time allocated to the user’s process?\n",
    "\n",
    "### **Answer**\n",
    "A user can structure their process to **mimic an interactive (I/O-bound) process** to be placed in a higher-priority queue.\n",
    "\n",
    "### **Detailed Explanation**\n",
    "Multilevel queue schedulers typically have:\n",
    "- **High-priority queues** for interactive processes (short time quanta, frequent scheduling).\n",
    "- **Low-priority queues** for batch/CPU-bound processes (long time quanta, infrequent scheduling).\n",
    "\n",
    "**Strategies:**\n",
    "1. **Frequent I/O Operations:** Insert periodic I/O calls (e.g., small writes, sleep calls) to make the process appear I/O-bound, favoring placement in a higher-priority queue.\n",
    "2. **Exploit Priority Rules:** If the system allows user-set priorities (e.g., `nice` values), set the process to highest allowed priority.\n",
    "3. **Process Characteristics:** Design the program to have short CPU bursts (< time quantum) to avoid being demoted to lower-priority queues in a **multilevel feedback queue** system.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.24: Dynamic Priority Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider a preemptive priority scheduling algorithm based on dynamically changing priorities. Larger priority numbers imply higher priority. When a process is waiting for the CPU (in the ready queue, but not running), its priority changes at a rate α. When it is running, its priority changes at a rate β. All processes are given a priority of 0 when they enter the ready queue. The parameters α and β can be set to give many different scheduling algorithms.\n",
    "\n",
    "a. What is the algorithm that results from β > α > 0?\n",
    "b. What is the algorithm that results from α < β < 0?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. β > α > 0 (Both positive, β larger)**\n",
    "- Running processes' priorities increase faster than waiting processes'.\n",
    "- Once a process starts running, it quickly becomes the highest priority and tends to keep the CPU.\n",
    "- This approximates **First-Come, First-Served (FCFS)** or **nonpreemptive scheduling**, as the initial process to run will retain control.\n",
    "\n",
    "#### **b. α < β < 0 (Both negative, α more negative)**\n",
    "- Priorities decrease (since negative rates), but waiting processes' priorities drop faster.\n",
    "- A running process retains a relatively higher priority compared to waiting processes.\n",
    "- New processes (priority 0) have higher priority than older waiting processes (negative priorities), leading to **Last-Come, First-Served (LCFS)** behavior, possibly with starvation of older processes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.25: Discrimination for Short Processes**\n",
    "\n",
    "### **Question**\n",
    "Explain how the following scheduling algorithms discriminate either in favor of or against short processes:\n",
    "\n",
    "a. FCFS\n",
    "b. RR\n",
    "c. Multilevel feedback queues\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. FCFS (First-Come, First-Served)**\n",
    "- **Discriminates against short processes.**\n",
    "- **Why:** The **convoy effect**—a short process arriving after a long process must wait for the long process to complete, increasing its waiting time disproportionately.\n",
    "\n",
    "#### **b. RR (Round Robin)**\n",
    "- **Treats all processes equally, but short processes benefit.**\n",
    "- **Why:** Each process gets equal time slices; short processes finish earlier because they require fewer total slices, reducing their turnaround time.\n",
    "\n",
    "#### **c. Multilevel Feedback Queues (MFQ)**\n",
    "- **Generally favors short (I/O-bound) processes.**\n",
    "- **Why:** Processes with short CPU bursts are promoted to higher-priority queues, getting more frequent CPU access. Long CPU-bound processes are demoted to lower-priority queues.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.26: Shared Ready Queue in SMP**\n",
    "\n",
    "### **Question**\n",
    "Describe why a shared ready queue might suffer from performance problems in an SMP environment.\n",
    "\n",
    "### **Answer**\n",
    "A shared ready queue becomes a **contention bottleneck** due to:\n",
    "1. **Lock Synchronization:** Multiple processors must lock the queue to enqueue/dequeue processes, causing serialization and spin-waiting.\n",
    "2. **Cache Coherence Overhead:** Frequent updates to the shared queue cause cache invalidation across processors, increasing memory latency.\n",
    "3. **Scalability Limit:** As processor count grows, contention increases, reducing parallel efficiency.\n",
    "\n",
    "**Solution:** Use per-processor run queues with work-stealing or load-balancing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.27: Load Balancing with Priority-Based Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider a load-balancing algorithm that ensures that each queue has approximately the same number of threads, independent of priority. How effectively would a priority-based scheduling algorithm handle this situation if one run queue had all high-priority threads and a second queue had all low-priority threads?\n",
    "\n",
    "### **Answer**\n",
    "It would handle it **poorly**. The load balancer ignores priority, leading to:\n",
    "- **Processor 1 (high-priority queue):** Responsive for high-priority threads.\n",
    "- **Processor 2 (low-priority queue):** Busy with low-priority threads, while high-priority threads might be waiting if Processor 1 is overloaded.\n",
    "- **Inefficiency:** Processor 2 could be executing low-priority threads while high-priority threads exist, violating priority-based scheduling goals.\n",
    "\n",
    "**Solution:** Load balancing should consider both thread count and priority, distributing high-priority threads across processors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.28: Process Placement in Per-Processor Run Queues**\n",
    "\n",
    "### **Question**\n",
    "Assume that an SMP system has private, per-processor run queues. When a new process is created, it can be placed in either the same queue as the parent process or a separate queue.\n",
    "\n",
    "a. What are the benefits of placing the new process in the same queue as its parent?\n",
    "b. What are the benefits of placing the new process in a different queue?\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Same Queue as Parent**\n",
    "- **Cache Affinity:** Parent and child likely share code/data; keeping them on the same processor improves cache hit rates.\n",
    "- **Synchronization Efficiency:** If they communicate frequently, shared memory accesses are faster within the same processor’s cache.\n",
    "\n",
    "#### **b. Different Queue**\n",
    "- **Load Distribution:** Spreads work across processors, improving overall throughput.\n",
    "- **Parallel Execution:** Allows true concurrency if parent and child are independent.\n",
    "- **Fault Isolation:** A busy processor doesn’t delay both processes.\n",
    "\n",
    "**Trade-off:** Systems often use **affinity scheduling** (same queue initially) with **migration** for load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b466c6a",
   "metadata": {},
   "source": [
    "\n",
    "## **Exercise 5.29: NUMA-Aware Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Assume that a thread has blocked for network I/O and is eligible to run again. Describe why a NUMA-aware scheduling algorithm should reschedule the thread on the same CPU on which it previously ran.\n",
    "\n",
    "### **Answer**\n",
    "In a **NUMA (Non-Uniform Memory Access)** system, memory access time depends on the memory location relative to the processor. Each processor has **local memory** (fast access) and **remote memory** (slower access across interconnects).\n",
    "\n",
    "**Reasons for Rescheduling on Same CPU:**\n",
    "\n",
    "1. **Memory Locality:** The thread's memory pages were likely allocated in the **local memory** of the CPU it was running on. Rescheduling it on the same CPU ensures fast access to its working set.\n",
    "2. **Cache Warmth:** The CPU's caches may still contain the thread's data and instructions, reducing cache miss penalties.\n",
    "3. **Reduced NUMA Penalty:** If scheduled on a different CPU, the thread would experience:\n",
    "   - **Remote memory accesses** (3-5x slower than local)\n",
    "   - **Complete cache cold start** (all cache misses)\n",
    "   - **Increased memory contention** on remote memory controllers\n",
    "\n",
    "**Performance Impact:** In NUMA systems, incorrect thread placement can degrade performance by 30-50% due to remote memory access latency.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.30: Windows Thread Priority Calculation**\n",
    "\n",
    "### **Question**\n",
    "Using the Windows scheduling algorithm, determine the numeric priority of each of the following threads.\n",
    "\n",
    "Windows uses:\n",
    "- **Priority Classes** (base priorities): IDLE (4), BELOW_NORMAL (6), NORMAL (8), ABOVE_NORMAL (10), HIGH (13), REALTIME (24)\n",
    "- **Relative Priorities** within class: IDLE (-2), BELOW_NORMAL (-1), NORMAL (0), ABOVE_NORMAL (+1), HIGHEST (+2), TIME_CRITICAL (+3 for non-realtime, +16 for realtime)\n",
    "\n",
    "**Numeric Priority = Base(Priority Class) + Relative Priority**\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. REALTIME PRIORITY CLASS with NORMAL relative priority**\n",
    "- REALTIME base = 24\n",
    "- NORMAL relative = 0\n",
    "- **Numeric Priority = 24 + 0 = 24**\n",
    "\n",
    "#### **b. ABOVE NORMAL PRIORITY CLASS with HIGHEST relative priority**\n",
    "- ABOVE_NORMAL base = 10\n",
    "- HIGHEST relative = +2\n",
    "- **Numeric Priority = 10 + 2 = 12**\n",
    "\n",
    "#### **c. BELOW NORMAL PRIORITY CLASS with ABOVE NORMAL relative priority**\n",
    "- BELOW_NORMAL base = 6\n",
    "- ABOVE_NORMAL relative = +1\n",
    "- **Numeric Priority = 6 + 1 = 7**\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.31: Highest Possible Windows Priority**\n",
    "\n",
    "### **Question**\n",
    "Assuming that no threads belong to the REALTIME PRIORITY CLASS and that none may be assigned a TIME_CRITICAL priority, what combination of priority class and priority corresponds to the highest possible relative priority in Windows scheduling?\n",
    "\n",
    "### **Answer**\n",
    "**HIGH PRIORITY CLASS with HIGHEST relative priority**\n",
    "\n",
    "**Calculation:**\n",
    "- Without REALTIME class, the highest base priority is **HIGH** = 13\n",
    "- Without TIME_CRITICAL, the highest relative priority is **HIGHEST** = +2\n",
    "- **Numeric Priority = 13 + 2 = 15**\n",
    "\n",
    "**Alternative expression:** Threads in the HIGH priority class with HIGHEST relative adjustment have the highest possible priority under these constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.32: Solaris Time-Sharing Thread Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider the scheduling algorithm in Solaris for time-sharing threads.\n",
    "\n",
    "**Solaris Priority System:**\n",
    "- Priorities 0-59, higher number = higher priority\n",
    "- Time quantum = (priority ≤ 59) ? (20 + (priority/4)) ms : 200 ms\n",
    "- Priority adjustments:\n",
    "  - Uses entire quantum without blocking: priority decreases (penalized for being CPU-bound)\n",
    "  - Blocks before quantum expires: priority increases (rewarded for being I/O-bound)\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Time quantum for priority 15? Priority 40?**\n",
    "- **Priority 15:** Quantum = 20 + ⌊15/4⌋ = 20 + 3 = **23 ms**\n",
    "- **Priority 40:** Quantum = 20 + ⌊40/4⌋ = 20 + 10 = **30 ms**\n",
    "\n",
    "#### **b. Thread with priority 50 uses entire quantum without blocking**\n",
    "- Using entire quantum → **priority decreases**\n",
    "- Typical decrease: by 1-5 priority points (implementation specific)\n",
    "- **New priority likely in range 45-49** (probably 49 if decreasing by 1)\n",
    "\n",
    "#### **c. Thread with priority 20 blocks for I/O before quantum expires**\n",
    "- Blocking before quantum ends → **priority increases**\n",
    "- Typical increase: by 1-10 priority points (more if blocked early)\n",
    "- **New priority likely in range 21-30** (could be up to 30 for very early block)\n",
    "\n",
    "**Design Philosophy:** This implements a **multilevel feedback queue** that rewards I/O-bound (interactive) threads and penalizes CPU-bound threads.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.33: Linux CFS Scheduler and vruntime**\n",
    "\n",
    "### **Question**\n",
    "Assume two tasks, A (nice = -5) and B (nice = +5), running on Linux with CFS scheduler. Describe how their vruntime values vary in these scenarios:\n",
    "\n",
    "**CFS Basics:** \n",
    "- Each task has virtual runtime (vruntime) = actual runtime × weight factor\n",
    "- Lower nice = higher weight = vruntime increases slower\n",
    "- CFS always runs task with **lowest vruntime**\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **Scenario 1: Both A and B are CPU-bound**\n",
    "- Both constantly use CPU, accumulate actual runtime\n",
    "- A (nice = -5, higher priority) has **higher weight** → vruntime increases **slower** than B\n",
    "- B (nice = +5, lower priority) has **lower weight** → vruntime increases **faster**\n",
    "- **Result:** A's vruntime stays lower than B's, so A gets **more CPU time** (approximately 3:1 ratio due to weight differences)\n",
    "\n",
    "#### **Scenario 2: A is I/O-bound, B is CPU-bound**\n",
    "- A blocks frequently for I/O → accumulates little actual runtime → vruntime increases very slowly\n",
    "- B runs continuously → accumulates actual runtime quickly → vruntime increases rapidly\n",
    "- **Result:** A's vruntime stays very low, so when A becomes ready, it immediately preempts B. A gets excellent responsiveness.\n",
    "\n",
    "#### **Scenario 3: A is CPU-bound, B is I/O-bound**\n",
    "- A runs continuously → vruntime increases (but slower due to higher weight)\n",
    "- B blocks frequently → vruntime increases very slowly when it runs\n",
    "- **Result:** B's vruntime may become lower than A's when B wakes up, allowing B to preempt A. This ensures I/O-bound tasks remain responsive even with lower priority (nice +5).\n",
    "\n",
    "**Key Insight:** CFS achieves fairness by **comparing vruntime, not raw CPU time**, allowing it to properly handle mixed workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.34: Rate-Monotonic vs Earliest-Deadline-First**\n",
    "\n",
    "### **Question**\n",
    "Provide a specific circumstance that illustrates where rate-monotonic scheduling is inferior to earliest-deadline-first scheduling in meeting real-time process deadlines.\n",
    "\n",
    "### **Answer**\n",
    "**Scenario:**\n",
    "- **Task T1:** Period = 50 ms, Execution time = 25 ms, Deadline = 50 ms\n",
    "- **Task T2:** Period = 75 ms, Execution time = 30 ms, Deadline = 75 ms\n",
    "\n",
    "**Rate-Monotonic (RM) Analysis:**\n",
    "- Assigns higher priority to task with **shorter period** (T1)\n",
    "- T1 always preempts T2\n",
    "- Schedule T1 first (0-25), then T2 (25-55), but T1's next period starts at 50\n",
    "- **Problem:** At time 50, T1 preempts T2 (which started at 25 and needs until 55)\n",
    "- T1 runs 50-75, then T2 resumes 75-80 (but T2's deadline was 75!)\n",
    "- **Result:** T2 **misses deadline** at 75\n",
    "\n",
    "**Earliest-Deadline-First (EDF) Analysis:**\n",
    "- Always runs task with **earliest deadline**\n",
    "- Initially: T1 deadline = 50, T2 deadline = 75 → run T1 (0-25)\n",
    "- At 25: T1 next deadline = 50, T2 deadline = 75 → still run T1? Wait, need to check.\n",
    "- Actually, EDF would schedule: T1 (0-25), T2 (25-55), T1 (55-80)...\n",
    "- **Deadline check:** T2 completes at 55 < deadline 75 ✓, T1 completes at 80 < deadline 100 ✓\n",
    "\n",
    "**Conclusion:** RM fails this task set even though CPU utilization (25/50 + 30/75 = 0.5 + 0.4 = 0.9) is below the RM bound (~0.828 for 2 tasks). EDF successfully schedules it because EDF can achieve 100% utilization.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.35: Rate-Monotonic vs EDF Scheduling**\n",
    "\n",
    "### **Question**\n",
    "Consider two processes:\n",
    "- P₁: period p₁ = 50, execution time t₁ = 25\n",
    "- P₂: period p₂ = 75, execution time t₂ = 30\n",
    "\n",
    "a. Can these be scheduled using rate-monotonic scheduling? Illustrate with Gantt chart.\n",
    "b. Illustrate scheduling using earliest-deadline-first (EDF).\n",
    "\n",
    "### **Answer**\n",
    "\n",
    "#### **a. Rate-Monotonic Scheduling**\n",
    "**Priority:** P₁ has higher priority (shorter period = 50)\n",
    "\n",
    "**Gantt Chart (first 150 ms):**\n",
    "```\n",
    "Time:   0    25    50    75    100   125   150\n",
    "        |-----|-----|-----|-----|-----|-----|\n",
    "P1:     [====]     [====]     [====]     [====]\n",
    "        0-25        50-75      100-125\n",
    "P2:           [==============]           [====...\n",
    "                25-55 (preempted!)          125-155\n",
    "```\n",
    "**Detailed Breakdown:**\n",
    "- 0-25: P₁ runs (completes)\n",
    "- 25-50: P₂ runs (needs 30 total, gets 25 so far)\n",
    "- 50-55: P₁ preempts (higher priority), P₂ paused with 5 ms remaining\n",
    "- 55-75: P₁ completes its 25 ms burst\n",
    "- 75-80: P₂ resumes and completes its remaining 5 ms\n",
    "- But P₂'s first deadline was at 75! It completed at 80 → **MISSED DEADLINE**\n",
    "\n",
    "**Conclusion:** RM fails. Utilization = 25/50 + 30/75 = 0.5 + 0.4 = 0.9 > 0.828 (RM bound for 2 tasks), so guaranteed to fail.\n",
    "\n",
    "#### **b. Earliest-Deadline-First Scheduling**\n",
    "EDF always runs task with earliest absolute deadline.\n",
    "\n",
    "**Gantt Chart (first 150 ms):**\n",
    "```\n",
    "Time:   0    25    50    75    100   125   150\n",
    "        |-----|-----|-----|-----|-----|-----|\n",
    "P1:     [====]           [====]     [====]\n",
    "        0-25 (d=50)      55-80     105-130\n",
    "                         (d=100)    (d=150)\n",
    "P2:           [==============]     [======...\n",
    "                25-55               80-110\n",
    "                (d=75)              (d=150)\n",
    "```\n",
    "**Schedule:**\n",
    "- 0-25: P₁ runs (deadline 50)\n",
    "- 25-55: P₂ runs (deadline 75) - completes at 55 < 75 ✓\n",
    "- 55-80: P₁ runs (next instance, deadline 100) - completes at 80 < 100 ✓\n",
    "- 80-110: P₂ runs (next instance, deadline 150) - completes at 110 < 150 ✓\n",
    "- 105-130: P₁ runs (skipped? Actually P₁ at 100... Let's recalculate carefully)\n",
    "\n",
    "**Correct EDF Schedule:**\n",
    "Time 0: P₁(d50), P₂(d75) → run P₁ (0-25)\n",
    "Time 25: P₁ next d=100, P₂(d75) → run P₂ (25-55)\n",
    "Time 55: P₁(d100), P₂ next d=150 → run P₁ (55-80)\n",
    "Time 80: P₁ next d=150, P₂(d150) → tie, can run either (say P₂: 80-110)\n",
    "Time 110: P₁(d150) → run P₁ (110-135)\n",
    "All deadlines met ✓\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.36: Bounded Latency in Hard Real-Time Systems**\n",
    "\n",
    "### **Question**\n",
    "Explain why interrupt and dispatch latency times must be bounded in a hard real-time system.\n",
    "\n",
    "### **Answer**\n",
    "In **hard real-time systems**, missing a deadline can cause **catastrophic failure** (e.g., aircraft control, medical devices, nuclear plant control).\n",
    "\n",
    "**Two Critical Latencies:**\n",
    "\n",
    "1. **Interrupt Latency:** Time from interrupt signal to start of ISR\n",
    "   - Must be bounded to ensure timely response to external events\n",
    "   - Factors: OS may disable interrupts (critical sections), interrupt controller delays\n",
    "\n",
    "2. **Dispatch Latency:** Time from ISR completion to task start\n",
    "   - Must be bounded to ensure high-priority tasks run promptly\n",
    "   - Factors: Context switch time, scheduler decisions, cache flushing\n",
    "\n",
    "**Why Bounded/Boundable:**\n",
    "- **Predictability:** System must guarantee worst-case response time ≤ deadline\n",
    "- **Schedulability Analysis:** Mathematical analysis (e.g., RM, EDF) requires known maximum latencies\n",
    "- **Certification:** Safety-critical systems require formal verification of timing constraints\n",
    "\n",
    "**Example:** In fly-by-wire systems, control loop must complete within 1-2 ms. Unbounded latency could cause oscillation or loss of control.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 5.37: Heterogeneous Multiprocessing in Mobile Systems**\n",
    "\n",
    "### **Question**\n",
    "Describe the advantages of using heterogeneous multiprocessing in a mobile system.\n",
    "\n",
    "### **Answer**\n",
    "**Heterogeneous Multiprocessing (HMP)** uses cores with different performance/power characteristics (e.g., ARM big.LITTLE: few \"big\" high-performance cores + many \"LITTLE\" power-efficient cores).\n",
    "\n",
    "### **Advantages:**\n",
    "\n",
    "1. **Energy Efficiency:** \n",
    "   - Light tasks → LITTLE cores (low power)\n",
    "   - Heavy tasks → big cores (high performance)\n",
    "   - Dynamic migration balances performance needs with battery life\n",
    "\n",
    "2. **Thermal Management:**\n",
    "   - Spread heat generation across different core types\n",
    "   - Use LITTLE cores when thermal throttling required\n",
    "   - Prevents overheating and performance degradation\n",
    "\n",
    "3. **Performance Scaling:**\n",
    "   - Fine-grained control over performance/power trade-off\n",
    "   - Better than DVFS alone (dynamic voltage/frequency scaling)\n",
    "\n",
    "4. **Cost Optimization:**\n",
    "   - Mix of core types provides better performance/$ than homogeneous design\n",
    "   - Smaller LITTLE cores occupy less die area\n",
    "\n",
    "5. **Workload Matching:**\n",
    "   - Background tasks (email sync, notifications) → LITTLE cores\n",
    "   - Foreground tasks (gaming, camera) → big cores\n",
    "   - OS scheduler intelligently places threads based on requirements\n",
    "\n",
    "**Example:** Samsung Exynos/Qualcomm Snapdragon use HMP to achieve all-day battery life while providing burst performance when needed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
