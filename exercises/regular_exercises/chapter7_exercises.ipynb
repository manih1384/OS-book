{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19673c39",
   "metadata": {},
   "source": [
    "# **Chapter 7: Synchronization Examples – Practice Exercise Solutions**\n",
    "\n",
    "## **7.1 Multiple Locking Mechanisms in Windows and Linux**\n",
    "\n",
    "**Question:**  \n",
    "Explain why Windows and Linux implement multiple locking mechanisms. Describe the circumstances under which they use spinlocks, mutex locks, semaphores, and condition variables. In each case, explain why the mechanism is needed.\n",
    "\n",
    "**Answer:**  \n",
    "These operating systems provide different locking mechanisms depending on the application developers’ needs.  \n",
    "- **Spinlocks**: Used in **multiprocessor systems** for **short-duration** critical sections where the cost of context switching (sleep/wake) is higher than briefly spinning. They avoid scheduler overhead.  \n",
    "- **Mutex locks**: Used for general **mutual exclusion** when a resource may be held for longer periods. They block the thread (put it to sleep) if the lock is unavailable, saving CPU cycles. Some systems (like Solaris) use **adaptive mutexes** that spin briefly before blocking.  \n",
    "- **Semaphores**: Used for **counting resources** (e.g., limiting concurrent accesses) or synchronizing between threads/processes where a thread may need to wait for a signal from another. Useful for **long waits**.  \n",
    "- **Condition variables**: Used with mutexes for **complex synchronization patterns** (e.g., producer-consumer) where a thread must wait for a specific condition to become true. They allow efficient waiting without busy polling.\n",
    "\n",
    "Each mechanism addresses different performance and correctness trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.2 Slim Reader-Writer Locks in Windows**\n",
    "\n",
    "**Question:**  \n",
    "Windows provides a lightweight synchronization tool called slim reader–writer locks. Whereas most implementations of reader–writer locks favor either readers or writers, or perhaps order waiting threads using a FIFO policy, slim reader–writer locks favor neither readers nor writers, nor are waiting threads ordered in a FIFO queue. Explain the benefits of providing such a synchronization tool.\n",
    "\n",
    "**Answer:**  \n",
    "**Simplicity and speed**. Slim reader-writer locks avoid the overhead of fairness policies (like FIFO ordering) or favoring readers/writers, which require additional queue management and state tracking. This makes acquisition and release **faster** with lower memory overhead. They are appropriate when strict fairness is not required, but fast, lightweight read-write synchronization is needed—common in high-performance kernel or user-space code where lock contention is moderate.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.3 Replacing Binary Semaphore with Mutex Lock**\n",
    "\n",
    "**Question:**  \n",
    "Describe what changes would be necessary to the producer and consumer processes in Figure 7.1 and Figure 7.2 so that a mutex lock could be used instead of a binary semaphore.\n",
    "\n",
    "**Answer:**  \n",
    "The calls to `wait(mutex)` and `signal(mutex)` (where `mutex` is a binary semaphore) need to be replaced with the **mutex lock API** calls:  \n",
    "- `wait(mutex)` → `acquire(mutex)` or `lock(mutex)`  \n",
    "- `signal(mutex)` → `release(mutex)` or `unlock(mutex)`  \n",
    "\n",
    "The logic remains identical; only the synchronization primitive changes. The mutex lock provides the same mutual exclusion but is often implemented with support for ownership (to prevent a thread from unlocking a mutex it doesn’t hold) and may include priority inheritance features.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.4 Deadlock in Dining Philosophers Problem**\n",
    "\n",
    "**Question:**  \n",
    "Describe how deadlock is possible with the dining-philosophers problem.\n",
    "\n",
    "**Answer:**  \n",
    "If all philosophers simultaneously pick up their **left fork**, each will hold one fork and wait indefinitely for their **right fork**, which is held by the neighbor to their right. Since no philosopher puts down a fork until they eat, all are blocked forever—a **circular wait** condition leading to deadlock.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.5 Signaled vs. Non-Signaled States in Windows**\n",
    "\n",
    "**Question:**  \n",
    "Explain the difference between signaled and non-signaled states with Windows dispatcher objects.\n",
    "\n",
    "**Answer:**  \n",
    "- **Signaled state**: The object is **available**. A thread attempting to acquire it (e.g., via `WaitForSingleObject`) will **not block** and will proceed immediately. The object may represent a lock that is free, an event that has occurred, etc.  \n",
    "- **Non-signaled state**: The object is **unavailable**. A thread trying to acquire it will **block** until the object transitions to signaled (e.g., when a lock is released or an event is set).\n",
    "\n",
    "---\n",
    "\n",
    "## **7.6 Atomic Operations in Linux**\n",
    "\n",
    "**Question:**  \n",
    "Assume `val` is an atomic integer in a Linux system. What is the value of `val` after the following operations have been completed?  \n",
    "```\n",
    "atomic_set(&val,10);\n",
    "atomic_sub(8,&val);\n",
    "atomic_inc(&val);\n",
    "atomic_inc(&val);\n",
    "atomic_add(6,&val);\n",
    "atomic_sub(3,&val);\n",
    "```\n",
    "\n",
    "**Answer:**  \n",
    "Calculating sequentially:  \n",
    "1. `atomic_set(&val,10)` → val = 10  \n",
    "2. `atomic_sub(8,&val)` → val = 10 - 8 = 2  \n",
    "3. `atomic_inc(&val)` → val = 2 + 1 = 3  \n",
    "4. `atomic_inc(&val)` → val = 3 + 1 = 4  \n",
    "5. `atomic_add(6,&val)` → val = 4 + 6 = 10  \n",
    "6. `atomic_sub(3,&val)` → val = 10 - 3 = 7  \n",
    "\n",
    "**Final value of val is 7**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea9b60",
   "metadata": {},
   "source": [
    "# **Chapter 7: Synchronization Examples – Exercise Solutions**\n",
    "\n",
    "## **Exercise 7.12**\n",
    "**Question:**  \n",
    "Describe two kernel data structures in which race conditions are possible. Be sure to include a description of how a race condition can occur.\n",
    "\n",
    "**Answer:**  \n",
    "1. **Process/Thread Control Block (PCB/TCB) Linked List**  \n",
    "   - **Race Condition**: When a new process is created or terminated, the kernel must update the linked list of PCBs. If two CPUs simultaneously attempt to insert or remove a PCB (e.g., during `fork()` and `exit()`), they may both manipulate the `next` pointers concurrently, corrupting the list. For example, both might read the same head pointer, then update it to point to their own new PCB, causing one insertion to be lost or creating a loop.\n",
    "\n",
    "2. **Memory Allocation Buddy System Free List**  \n",
    "   - **Race Condition**: The buddy system maintains lists of free memory blocks of different sizes. When allocating or freeing a block, the kernel must remove or insert blocks from these lists. If two processes on different cores simultaneously request memory of the same size, they might both find the same free block in the list, both attempt to remove it, and both believe they have acquired it, leading to double allocation and potential memory corruption.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 7.13**\n",
    "**Question:**  \n",
    "The Linux kernel has a policy that a process cannot hold a spinlock while attempting to acquire a semaphore. Explain why this policy is in place.\n",
    "\n",
    "**Answer:**  \n",
    "This policy prevents **deadlock** and **priority inversion** scenarios.  \n",
    "- A spinlock is designed for **short-term** locking in contexts where sleeping is not allowed (e.g., interrupt handlers). Holding a spinlock and then trying to acquire a semaphore (which may **block/sleep** if unavailable) would cause the thread to sleep while holding the spinlock.  \n",
    "- If the thread sleeps, another thread on the same CPU might try to acquire the same spinlock, causing **deadlock** because the lock is held by a sleeping thread.  \n",
    "- Additionally, spinning with a held semaphore could waste CPU indefinitely if the semaphore is held by a low-priority process that gets preempted.  \n",
    "The policy enforces that spinlocks are only used in non-blocking contexts.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 7.14**\n",
    "**Question:**  \n",
    "Design an algorithm for a bounded-buffer monitor in which the buffers (portions) are embedded within the monitor itself.\n",
    "\n",
    "**Answer:**  \n",
    "Monitor with internal buffer array and condition variables for empty/full.\n",
    "\n",
    "```java\n",
    "monitor BoundedBuffer {\n",
    "    private int buffer[N];\n",
    "    private int count = 0, in = 0, out = 0;\n",
    "    condition notFull, notEmpty;\n",
    "\n",
    "    void produce(int item) {\n",
    "        while (count == N) {\n",
    "            notFull.wait();  // wait until buffer not full\n",
    "        }\n",
    "        buffer[in] = item;\n",
    "        in = (in + 1) % N;\n",
    "        count++;\n",
    "        notEmpty.signal();  // signal consumer\n",
    "    }\n",
    "\n",
    "    int consume() {\n",
    "        while (count == 0) {\n",
    "            notEmpty.wait(); // wait until buffer not empty\n",
    "        }\n",
    "        int item = buffer[out];\n",
    "        out = (out + 1) % N;\n",
    "        count--;\n",
    "        notFull.signal();   // signal producer\n",
    "        return item;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 7.15**\n",
    "**Question:**  \n",
    "The strict mutual exclusion within a monitor makes the bounded-buffer monitor of Exercise 7.14 mainly suitable for small portions.\n",
    "a. Explain why this is true.\n",
    "b. Design a new scheme that is suitable for larger portions.\n",
    "\n",
    "**Answer:**  \n",
    "**a.** Strict mutual exclusion means only one thread can be active inside the monitor at a time. If the buffer operations (`produce`/`consume`) involve **large data portions** (e.g., copying large chunks of memory), the critical section becomes long. This serializes all producers and consumers, hurting **throughput** and **parallelism** unnecessarily, since multiple producers could fill different buffer slots concurrently, and multiple consumers could take from different slots concurrently.\n",
    "\n",
    "**b.** **Scheme for larger portions**: Use a **reader-writer** style approach or **fine-grained locking**.\n",
    "- Split the buffer into **multiple slots**, each with its own lock (or use an array of semaphores).\n",
    "- Producers and consumers can operate on **different slots simultaneously**.\n",
    "- Maintain shared counters (`count`, `in`, `out`) with atomic operations or a separate lock.\n",
    "\n",
    "**Pseudocode using multiple semaphores:**\n",
    "```c\n",
    "semaphore mutex = 1;          // protects in/out/count\n",
    "semaphore empty = N;          // counts empty slots\n",
    "semaphore full = 0;           // counts full slots\n",
    "semaphore slot_locks[N] = {1,1,...}; // one per slot\n",
    "\n",
    "void produce(int item, int slot_index) {\n",
    "    wait(empty);              // ensure space\n",
    "    wait(mutex);\n",
    "    // find free slot (could be predefined), e.g., slot_index = in;\n",
    "    wait(slot_locks[slot_index]); // lock this slot only\n",
    "    buffer[slot_index] = item; // copy large data\n",
    "    update in, count;\n",
    "    signal(mutex);\n",
    "    signal(slot_locks[slot_index]);\n",
    "    signal(full);\n",
    "}\n",
    "\n",
    "int consume(int slot_index) {\n",
    "    wait(full);\n",
    "    wait(mutex);\n",
    "    // find full slot, e.g., slot_index = out;\n",
    "    wait(slot_locks[slot_index]);\n",
    "    int item = buffer[slot_index];\n",
    "    update out, count;\n",
    "    signal(mutex);\n",
    "    signal(slot_locks[slot_index]);\n",
    "    signal(empty);\n",
    "    return item;\n",
    "}\n",
    "```\n",
    "This allows concurrent data copying into different buffer slots.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 7.16**\n",
    "**Question:**  \n",
    "Discuss the tradeoff between fairness and throughput of operations in the readers–writers problem. Propose a method for solving the readers–writers problem without causing starvation.\n",
    "\n",
    "**Answer:**  \n",
    "**Tradeoff**:  \n",
    "- **Throughput favor (Reader priority)**: Allows multiple readers simultaneously, maximizing read throughput, but **writers may starve** if readers keep arriving.  \n",
    "- **Fairness (Writer priority or FIFO)**: Writers get timely access, preventing starvation, but may reduce read throughput because readers are serialized or forced to wait.  \n",
    "\n",
    "**Starvation-Free Solution**:  \n",
    "Use a **fair scheduling policy** like:\n",
    "1. **Timestamp ordering**: Arriving threads (readers/writers) get a timestamp. Service in order; a writer blocks all later arrivals; a reader allows other readers with earlier timestamps to join.\n",
    "2. **The \"No Starvation\" Readers-Writers Lock**:  \n",
    "   - Use two mutexes: `resource_mutex` (for writers) and `read_count_mutex` (for reader count).  \n",
    "   - Add a **turnstile** semaphore or a **queue semaphore** to enforce FIFO order for both readers and writers.  \n",
    "   - When a writer is waiting, new readers are blocked until the writer finishes.  \n",
    "   - Implementation often uses a **fair semaphore** (like `pthread_rwlock` with priority to writers) or **condition variables with a queue**.\n",
    "\n",
    "**Example using condition variables:**\n",
    "```java\n",
    "monitor FairRW {\n",
    "    int readers = 0;\n",
    "    bool writing = false;\n",
    "    int waitingWriters = 0;\n",
    "    condition okToRead, okToWrite;\n",
    "\n",
    "    void startRead() {\n",
    "        while (writing || waitingWriters > 0) {\n",
    "            okToRead.wait();\n",
    "        }\n",
    "        readers++;\n",
    "        okToRead.signal(); // cascade to other waiting readers\n",
    "    }\n",
    "\n",
    "    void endRead() {\n",
    "        readers--;\n",
    "        if (readers == 0) {\n",
    "            okToWrite.signal();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    void startWrite() {\n",
    "        waitingWriters++;\n",
    "        while (readers > 0 || writing) {\n",
    "            okToWrite.wait();\n",
    "        }\n",
    "        waitingWriters--;\n",
    "        writing = true;\n",
    "    }\n",
    "\n",
    "    void endWrite() {\n",
    "        writing = false;\n",
    "        if (waitingWriters > 0) {\n",
    "            okToWrite.signal();\n",
    "        } else {\n",
    "            okToRead.signal();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "This gives writers priority but ensures readers eventually proceed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 7.17**\n",
    "**Question:**  \n",
    "Explain why the call to the `lock()` method in a Java `ReentrantLock` is not placed in the `try` clause for exception handling, yet the call to the `unlock()` method is placed in a `finally` clause.\n",
    "\n",
    "**Answer:**  \n",
    "- `lock()` is not placed in `try` because if `lock()` throws an exception (e.g., `InterruptedException`), the lock has **not been acquired**, so we should **not** attempt to unlock it in `finally`. Unlocking without ownership would cause an `IllegalMonitorStateException`.  \n",
    "- `unlock()` is placed in `finally` to **guarantee** that the lock is released if it was successfully acquired, even if an exception occurs in the critical section. This prevents deadlock.\n",
    "\n",
    "**Typical pattern:**\n",
    "```java\n",
    "ReentrantLock lock = new ReentrantLock();\n",
    "lock.lock();  // outside try\n",
    "try {\n",
    "    // critical section\n",
    "} finally {\n",
    "    lock.unlock(); // always unlock if locked\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 7.18**\n",
    "**Question:**  \n",
    "Explain the difference between software and hardware transactional memory.\n",
    "\n",
    "**Answer:**  \n",
    "- **Software Transactional Memory (STM)**: Implemented entirely in software (libraries, compiler support). It uses **logging**, **versioning**, or **copy-on-write** to group memory operations into atomic transactions. Conflicts are detected via software algorithms (e.g., using locks or timestamps). Higher overhead but portable across hardware.\n",
    "\n",
    "- **Hardware Transactional Memory (HTM)**: Supported directly by CPU hardware (e.g., Intel TSX, IBM POWER). Uses **cache coherence protocols** to detect conflicts and CPU caches to buffer speculative writes. Transactions are executed speculatively; if a conflict occurs (another core accesses same memory), hardware aborts and rolls back. Lower overhead, faster for small transactions, but limited by hardware constraints (cache size, instruction support).\n",
    "\n",
    "**Key difference**: HTM is faster and transparent to programmer but limited in transaction size and not universally available. STM is more flexible (larger transactions) but slower due to software overhead.\n",
    "\n",
    "--- \n",
    "\n",
    "**End of Chapter 7 Exercises**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
