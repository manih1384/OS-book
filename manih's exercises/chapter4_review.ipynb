{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81babd37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **4.2 Amdahl's Law Calculation**\n",
    "\n",
    "**Question:** Using Amdahl’s Law, calculate the speedup gain of an application that has a 60 percent parallel component for (a) two processing cores and (b) four processing cores.\n",
    "\n",
    "**Answer:**\n",
    "a. With two processing cores, we get a speedup of 1.42 times.\n",
    "b. With four processing cores, we get a speedup of 1.82 times.\n",
    "\n",
    "### **4.3 Type of Parallelism in a Web Server**\n",
    "\n",
    "**Question:** Does the multithreaded web server described in Section 4.1 exhibit task or data parallelism?\n",
    "\n",
    "**Answer:** It exhibits data parallelism. This is because each thread is performing the identical task (servicing a request), but it is operating on different data associated with each client.\n",
    "\n",
    "### **4.4 User-Level vs. Kernel-Level Threads**\n",
    "\n",
    "**Question:** What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the other?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "\n",
    "a. The kernel is unaware of user-level threads, which are managed by a thread library in user space. In contrast, the kernel is directly aware of and manages kernel-level threads.\n",
    "\n",
    "\n",
    "b. User-level threads are scheduled by the thread library, while kernel-level threads are scheduled directly by the operating system kernel.\n",
    "\n",
    "\n",
    "c. Kernel threads are generally more expensive to create and manage because each one must be represented by a kernel data structure. User-level threads are better for fine-grained tasks and fast thread creation due to their low overhead. Kernel-level threads are better when true parallel execution on multiple processors is required, as the kernel can schedule them concurrently.\n",
    "\n",
    "### **4.5 Kernel Thread Context Switching**\n",
    "\n",
    "**Question:** Describe the actions taken by a kernel to context-switch between kernel-level threads.\n",
    "\n",
    "**Answer:** Context switching between kernel threads requires the kernel to save the current state of the CPU registers for the thread that is being switched out. It then restores the previously saved register state for the new thread that is being scheduled to run.\n",
    "\n",
    "### **4.6 Resource Usage: Threads vs. Processes**\n",
    "\n",
    "**Question:** What resources are used when a thread is created? How do they differ from those used when a process is created?\n",
    "\n",
    "**Answer:** Thread creation uses significantly fewer resources than process creation. Creating a process requires allocating a large Process Control Block (PCB), which includes a memory map, list of open files, and environment variables. Allocating the memory map is particularly time-consuming. In contrast, creating a thread only involves allocating a small data structure to hold a register set, stack, and priority.\n",
    "\n",
    "### **4.7 Binding Real-Time Threads to LWPs**\n",
    "\n",
    "**Question:** Assume that an operating system maps user-level threads to the kernel using the many-to-many model and that the mapping is done through LWPs. Furthermore, the system allows developers to create real-time threads for use in real-time systems. Is it necessary to bind a real-time thread to an LWP? Explain.\n",
    "\n",
    "**Answer:** Yes, it is necessary to bind a real-time thread to an LWP. For real-time applications, guaranteed and immediate scheduling is critical. If a real-time thread is not bound, it may be forced to wait for an available LWP to become attached to before it can run after being scheduled. This unbounded waiting period is unacceptable for real-time constraints. By binding an LWP to the real-time thread, you ensure the thread can begin execution with minimal delay the moment it is scheduled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f495b5",
   "metadata": {},
   "source": [
    "\n",
    "### **4.8 Poor Multithreading Performance Examples**\n",
    "\n",
    "**Question:** Provide two programming examples in which multithreading does not provide better performance than a single-threaded solution.\n",
    "\n",
    "**Answer:**\n",
    "a. Any simple, sequential task where the overhead of thread creation, context switching, and synchronization is greater than the performance benefit of parallel execution. For example, a program that calculates the sum of a small array.\n",
    "b. A task that is inherently sequential, where each step depends on the result of the previous one, such as computing the Fibonacci sequence recursively or a linked list traversal where each node points to the next.\n",
    "\n",
    "\n",
    "### **4.10 Shared State in a Multithreaded Process**\n",
    "\n",
    "**Question:** Which of the following components of program state are shared across threads in a multithreaded process?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a. Register values\n",
    "\n",
    "\n",
    "b. Heap memory\n",
    "\n",
    "\n",
    "\n",
    "c. Global variables\n",
    "\n",
    "\n",
    "\n",
    "d. Stack memory\n",
    "\n",
    "**Answer:**\n",
    "b. Heap memory and c. Global variables are shared across threads. Each thread has its own a. Register values and d. Stack memory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **4.11 User Threads on a Multiprocessor System**\n",
    "\n",
    "**Question:** Can a multithreaded solution using multiple user-level threads achieve better performance on a multiprocessor system than on a single-processor system? Explain.\n",
    "\n",
    "**Answer:** Typically, no. Since the operating system kernel is only aware of the single process and its associated kernel thread, it cannot schedule multiple user-level threads onto different processors concurrently. The entire process, including all its user-level threads, is scheduled on a single processor. Therefore, the parallelism cannot be exploited across multiple CPUs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **4.12 Chrome: Processes vs. Threads for Tabs**\n",
    "\n",
    "**Question:** In Chapter 3, we discussed Google’s Chrome browser and its practice of opening each new tab in a separate process. Would the same benefits have been achieved if, instead, Chrome had been designed to open each new tab in a separate thread? Explain.\n",
    "\n",
    "**Answer:** No, the key benefits would not be achieved. The main benefits of using separate processes are:\n",
    "*   **Stability and Isolation:** A crash in one tab (process) does not bring down the entire browser, as processes have separate memory spaces. Threads within a single process share memory, so a bug in one thread could corrupt memory and crash all tabs.\n",
    "*   **Security:** The operating system can enforce stricter security boundaries (sandboxing) between processes than between threads.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **4.14 Amdahl's Law Calculations**\n",
    "\n",
    "**Question:** Using Amdahl’s Law, calculate the speedup gain for the following applications:\n",
    "*   40 percent parallel with (a) eight processing cores and (b) sixteen processing cores\n",
    "*   67 percent parallel with (a) two processing cores and (b) four processing cores\n",
    "*   90 percent parallel with (a) four processing cores and (b) eight processing cores\n",
    "\n",
    "**Answer:**\n",
    "*   **40% Parallel:** Speedup = 1 / ((1 - 0.4) + (0.4 / N))\n",
    "    *   (a) With 8 cores: **~1.54 times**\n",
    "    *   (b) With 16 cores: **~1.60 times**\n",
    "*   **67% Parallel:** Speedup = 1 / ((1 - 0.67) + (0.67 / N))\n",
    "    *   (a) With 2 cores: **~1.33 times**\n",
    "    *   (b) With 4 cores: **~1.60 times**\n",
    "*   **90% Parallel:** Speedup = 1 / ((1 - 0.9) + (0.9 / N))\n",
    "    *   (a) With 4 cores: **~3.08 times**\n",
    "    *   (b) With 8 cores: **~4.71 times**\n",
    "\n",
    "\n",
    "\n",
    "### **4.15 Task vs. Data Parallelism**\n",
    "\n",
    "**Question:** Determine if the following problems exhibit task or data parallelism:\n",
    "*   Using a separate thread to generate a thumbnail for each photo in a collection\n",
    "*   Transposing a matrix in parallel\n",
    "*   A networked application where one thread reads from the network and another writes to the network\n",
    "*   The fork-join array summation application described in Section 4.5.2\n",
    "*   The Grand Central Dispatch system\n",
    "\n",
    "**Answer:**\n",
    "*   **Generating thumbnails:** **Data parallelism.** The same task (generate a thumbnail) is applied to different data (each photo).\n",
    "*   **Transposing a matrix:** **Data parallelism.** Different threads work on different sections (rows/columns/blocks) of the same matrix to perform the identical transposition operation.\n",
    "*   **Networked application:** **Task parallelism.** The two threads are performing different, specialized tasks (reading and writing).\n",
    "*   **Fork-join array summation:** **Data parallelism.** The array is split into parts, and the same summation task is performed on each part concurrently.\n",
    "*   **Grand Central Dispatch:** **Both.** It is a system designed to manage both types. It can execute different, unrelated tasks (task parallelism) and also divide a large dataset to be processed by multiple threads (data parallelism).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835cbf3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **4.16 Threading Strategy for a CPU-Intensive Application**\n",
    "\n",
    "**Question:** A system with two dual-core processors has four processors available for scheduling. A CPU-intensive application is running on this system. All input is performed at program start-up, when a single file must be opened. Similarly, all output is performed just before the program terminates, when the program results must be written to a single file. Between start-up and termination, the program is entirely CPU-bound. Your task is to improve the performance of this application by multithreading it. The application runs on a system that uses the one-to-one threading model (each user thread maps to a kernel thread).\n",
    "- How many threads will you create to perform the input and output? Explain.\n",
    "- How many threads will you create for the CPU-intensive portion of the application? Explain.\n",
    "\n",
    "**Answer:**\n",
    "- **Input and Output:** A single thread for each. Since I/O involves a single file for both operations, using one thread for input and one for output is sufficient. Creating more I/O threads would not speed up access to a single sequential file and could even slow it down due to contention.\n",
    "- **CPU-intensive portion:** Four threads. Because the system has four processing cores and uses a one-to-one threading model, creating four threads allows the operating system to schedule each thread on a separate core simultaneously, maximizing parallel execution and CPU utilization.\n",
    "\n",
    "### **4.17 Process and Thread Creation Analysis**\n",
    "\n",
    "**Question:** Consider the following code segment:\n",
    "```c\n",
    "pid_t pid;\n",
    "pid = fork();\n",
    "if (pid == 0) { /* child process */\n",
    "    fork();\n",
    "    thread_create( . . .);\n",
    "}\n",
    "fork();\n",
    "```\n",
    "a. How many unique processes are created?\n",
    "b. How many unique threads are created?\n",
    "\n",
    "**Answer:**\n",
    "a. **5 unique processes** are created.\n",
    "b. **1 unique thread** is created (in addition to the original main thread in that process).\n",
    "\n",
    "### **4.18 Kernel Process-Thread Models: Linux vs. Windows**\n",
    "\n",
    "**Question:** As described in Section 4.7.2, Linux does not distinguish between processes and threads. Instead, Linux treats both in the same way, allowing a task to be more akin to a process or a thread depending on the set of flags passed to the clone() system call. However, other operating systems, such as Windows, treat processes and threads differently. Typically, such systems use a notation in which the data structure for a process contains pointers to the separate threads belonging to the process. Contrast these two approaches for modeling processes and threads within the kernel.\n",
    "\n",
    "**Answer:**\n",
    "The contrast lies in the fundamental data structure used:\n",
    "- **Linux's Unified Approach:** Linux uses a single data structure (`task_struct`) to represent both processes and threads. A \"process\" is a task that does not share most of its resources (like memory maps) with its parent. A \"thread\" is a task that shares most of its resources (created with the `CLONE_VM`, `CLONE_FS`, and `CLONE_FILES` flags). This design is simpler and more uniform.\n",
    "- **Windows' Differentiated Approach:** Windows explicitly distinguishes between processes and threads. An `EPROCESS` block represents a process and contains its resource list, while separate `ETHREAD` blocks represent each thread belonging to that process. The `EPROCESS` block points to these threads. This model creates a clearer conceptual and structural separation between the resource container (process) and the units of execution (threads).\n",
    "\n",
    "### **4.19 Pthreads and Fork Output Analysis**\n",
    "\n",
    "**Question:** The program shown in the following code uses the Pthreads API. What would be the output from the program at LINE C and LINE P?\n",
    "\n",
    "```c\n",
    "#include <pthread.h>\n",
    "#include <stdio.h>\n",
    "int value = 0;\n",
    "void *runner(void *param); /* the thread */\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    pid_t pid;\n",
    "    pthread_t tid;\n",
    "    pthread_attr_t attr;\n",
    "    pid = fork();\n",
    "\n",
    "    if (pid == 0) { /* child process */\n",
    "        pthread_attr_init(&attr);\n",
    "        pthread_create(&tid, &attr, runner, NULL);\n",
    "        pthread_join(tid, NULL);\n",
    "        printf(\"CHILD: value = %d\\n\", value); /* LINE C */\n",
    "    }\n",
    "    else if (pid > 0) { /* parent process */\n",
    "        wait(NULL);\n",
    "        printf(\"PARENT: value = %d\\n\", value); /* LINE P */\n",
    "    }\n",
    "}\n",
    "\n",
    "void *runner(void *param) {\n",
    "    value = 5;\n",
    "    pthread_exit(0);\n",
    "}\n",
    "```\n",
    "\n",
    "**Answer:**\n",
    "- **LINE C (CHILD):** `value = 5`. The child process creates a thread that changes the value in the child's memory space to 5.\n",
    "- **LINE P (PARENT):** `value = 0`. The `fork()` call creates a duplicate but separate memory space for the child. The change made by the thread in the child process does not affect the parent process's memory. The parent's `value` remains 0.\n",
    "\n",
    "### **4.20 Many-to-Many Threading Performance**\n",
    "\n",
    "**Question:** Consider a multicore system and a multithreaded program written using the many-to-many threading model. Let the number of user-level threads in the program be greater than the number of processing cores in the system. Discuss the performance implications of the following scenarios.\n",
    "a. The number of kernel threads allocated to the program is less than the number of processing cores.\n",
    "b. The number of kernel threads allocated to the program is equal to the number of processing cores.\n",
    "c. The number of kernel threads allocated to the program is greater than the number of processing cores but less than the number of user-level threads.\n",
    "\n",
    "**Answer:**\n",
    "a. **Suboptimal Performance:** The program cannot fully utilize the available hardware. Some CPU cores will sit idle because there are not enough kernel threads to schedule on them, leading to under-utilization and poorer performance.\n",
    "b. **Optimal CPU Utilization:** This scenario allows the program to fully saturate all available CPU cores. The thread library can map multiple user threads onto these kernel threads, and the OS can schedule one kernel thread on each core, maximizing parallel execution.\n",
    "c. **Potential for Overhead:** This allows all CPU cores to be kept busy (as in scenario b). However, having more kernel threads than cores introduces the overhead of context switching between them by the OS scheduler. While this can be beneficial for I/O-bound tasks, for a CPU-bound workload, this extra context switching can slightly reduce performance compared to scenario (b).\n",
    "\n",
    "### **4.21 Thread Cancellation Safety**\n",
    "\n",
    "**Question:** Pthreads provides an API for managing thread cancellation. The `pthread_setcancelstate()` function is used to set the cancellation state. Using the code segment shown below, provide examples of two operations that would be suitable to perform between the calls to disable and enable thread cancellation.\n",
    "```c\n",
    "int oldstate;\n",
    "pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, &oldstate);\n",
    "/* What operations would be performed here? */\n",
    "pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &oldstate);\n",
    "```\n",
    "\n",
    "**Answer:**\n",
    "Operations that must be completed atomically without being interrupted by cancellation are suitable for this critical section. Two examples are:\n",
    "1. **Acquiring a Mutex Lock:** To prevent the thread from being canceled while holding a mutex, which would cause a deadlock for other threads waiting on that mutex.\n",
    "2. **Updating a shared data structure:** To ensure the data structure is left in a consistent, uncorrupted state if the thread is canceled later. The update is performed entirely before cancellation is re-enabled."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
